---
title: "\\vspace{5cm}**\\textcolor{blue}{Series Temporales} - Máster en Ciencia de
  Datos**"
author: "\\vspace{2cm} Anna Martínez-Gavara."
header-includes:
- \usepackage[spanish]{babel}.
- \usepackage[utf8]{inputenc}
- \usepackage{amsmath,amssymb,amsfonts}
- \usepackage{color}
- \usepackage{xcolor}
- \usepackage{graphicx}
- \usepackage{eqnarray}
output:
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
    citation_package: natbib
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", out.width = "0.6\\textwidth",message=FALSE, warning=FALSE)
```

\newpage


```{r}
library(ggplot2)
library(readr)
library(forecast)
library(dplyr)
```

**Carga del DataFrame**


```{r}
# Carga de dataset 

Dat <- read.csv(file = "./organised_Gen.csv", header = TRUE, sep= ",", dec = ".")

# Eliminar primera columna

Dat <- Dat %>%
  select(-X)

# Vista del DataFrame

columns_to_check <- names(Dat)[1:(ncol(Dat) - 1)]
for (column in columns_to_check) {
  unique_values <- unique(Dat[[column]])
  num_unique_values <- length(unique_values)
  cat(column, ": ", unique_values, " || Total Values:", num_unique_values, "\n\n")
}

# llevo fecha a primera columna solo

Dat <- Dat %>%
  mutate(DATE = as.Date(paste(YEAR, MONTH, "01", sep = "-"), format = "%Y-%m-%d")) %>%
  select(DATE, everything()) %>%
  select(-YEAR, -MONTH)


```

**Total de Estados Unidos**

\vspace*{2mm}

```{r}

Total_gen <- filter(Dat, STATE == "US-TOTAL")

total_by_producer <- Total_gen %>%
  group_by(DATE, TYPE.OF.PRODUCER) %>%
  summarise(across(where(is.numeric), sum)) %>%
  ungroup()


ggplot(total_by_producer, aes(x = plot1$DATE, y = plot1$GENERATION..Megawatthours., color = plot1$TYPE.OF.PRODUCER)) +
  geom_line() +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  theme_minimal() +
  labs(
    title = "Total energy generated by type of producer",
    x = "Year",
    y = "Generation (TWh)",
    color = "Type"
  ) +
  scale_color_brewer(palette = "Set1") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(legend.position = "top", legend.box = "horizontal", legend.text = element_text(size = 8)) +
  guides(color = guide_legend(title = "Type"))



# Dat_Hipotecas <- read.csv(file="Hipotecas.csv",header=TRUE,sep=";")
# attach(Dat_Hipotecas)
# boxplot(Hipotecas~Mes)
# 
# Hipotecas_ts <- ts(Hipotecas,start=c(2003,1),end=c(2018,12),frequency=12) 
# # Creamos una serie temporal, con 12 observaciones por unidad de tiempo
# 
# plot(Hipotecas_ts,xlab="A?o_Mes",ylab="Num de hipotecas")
```

**Ejemplo 4**: Encuesta de ocupaci?n hotelera. Total nacional de viajeros residentes en Espa?a. Fuente: Instituto Nacional de Estad?stica. 

\vspace*{2mm}
```{r}
Dat_Viajeros <- read.csv(file="Viajeros.csv",header=TRUE,sep=";")
attach(Dat_Viajeros)
boxplot(Viajeros~Mes)

Viajeros_ts <- ts(Viajeros,start=c(2010,1),end=c(2018,12),frequency=12) 
# Creamos una serie temporal, con 12 observaciones por unidad de tiempo

plot(Viajeros_ts,xlab="A?o_Mes",ylab="Viajeros residentes en Espa?a")
```

**Ejemplo 5**: Tr?fico comercial, aeropuerto de Valencia. Fuente: Ministerio de transportes, movilidad y agenda urbana (https://www.fomento.gob.es/BE/?nivel=2&orden=03000000).  

\vspace*{2mm}
```{r}
Dat_Pasajeros <- read.csv(file="Pasajeros.csv",header=TRUE,sep=";")
attach(Dat_Pasajeros)
T <- length(Total)
Pasajeros_ts <- ts(Total[121:T],start=c(2010,1),end=c(2019,12),frequency=12) 

plot(Pasajeros_ts,xlab="A?o_Mes",ylab="Num Pasajeros")
```

## Clasificaci?n de series temporales

Como hemos mencionado anteriormente, una serie temporal es una sucesi?n de observaciones ordenadas cronol?gicamente: $\{x_t\}_{t=1}^{n}$. Podemos entender entonces que la serie temporal de estudio es una realizaci?n de un proceso estoc?stico, definido como una sucesi?n de variables aleatorias que evolucionan en funci?n del tiempo: $\{X_t\}_{t=1}^{n}$. Cada una de estas variables aleatorias $X_t$ tiene su propia funci?n de distribuci?n y pueden o no estar correlacionadas entre s?. 

Representaremos por $\mu_t$ y $\sigma^{2}_t$ la esperanza y la varianza de $X_t$; es decir:
$$\begin{array}{ccl}
\mu_t & = & E(X_t)\\
\sigma^{2}_t &=& V(X_t) = E[(X_t - \mu_t)^2].
\end{array}$$

As? mismo, se define la covarianza entre dos variables aleatorias como:
$$
\gamma(t,s) = E[(X_t - \mu_t)(X_s - \mu_s)].
$$

Decimos que una serie temporal es \textcolor{blue}{estacionaria en sentido amplio} (o d?bilmente estacionaria) si la media y la varianza son independientes del tiempo y la covarianza entre dos variables s?lo depende del lapso de tiempo transcurrido entre los dos periodos y no depende del tiempo:
$$ \begin{array}{ccl}
\mu_t & = & \mu \\
\sigma^{2}_t & = & \sigma^{2}\\
\gamma(t,t+k) & = & \gamma_k
\end{array}$$

Una serie cuya media y/o variabilidad cambian a lo largo del tiempo es, por tanto,  \textcolor{blue}{no estacionaria}. Una serie no estacionaria puede mostrar cambios de varianza (heteroced?stica), puede mostrar una tendencia (cambios en la media a lo largo del tiempo), o puede presentar efectos estacionales (comportamiento parecido en determinados instantes temporales).

El ejemplo m?s sencillo de serie temporal estacionaria es el ruido blanco: media y covarianza son siempre cero. 

Algunas de las ventajas que presentan las series estacionarias son:
\begin{itemize}
\item Podemos obtener predicciones f?cilmente: $\hat{x}_{t+h} = \mu$.
\item Como la media es constante, podemos utilizar todos los valores observados para estimarla.
\end{itemize}

## Componentes de una serie temporal

En muchos casos se supone que el comportamiento de la variable en el tiempo es la integraci?n de varias componentes. Por ejemplo, $X_t = f(T_t,S_t,\epsilon_t)$, donde:
\begin{itemize}
\item $T_t$: \textcolor{blue}{Tendencia}. Comportamiento o movimiento suave de la serie temporal a largo plazo.
\item $S_t$: \textcolor{blue}{Estacionalidad}. Oscilaciones peri?dicas dentro de cada a?o.  Denotamos por $c$ la longitud del ciclo estacional ($c = 12$ datos mensuales, $c=4$ datos trimestrales, etc).
\item $\epsilon_t$: \textcolor{blue}{Residuo} (o efecto aleatorio). Variaci?n aleatoria alrededor de las componentes anteriores.
\end{itemize}

En ocasiones, se introduce una componente c?clica, que recoge comportamientos recurrentes con un periodo superior a un a?o. Esta componente c?clica suele ser dificil de reconocer y, generalmente, no se consigue separar de la tendencia.   

Seg?n como act?en estas componentes, podemos distinguir entre:
\begin{itemize}
\item Series temporales aditivas:
$$ x_t = T_t + S_t + \epsilon_t$$
\item Series temporales multiplicativas:
$$ x_t = T_t \cdot S_t \cdot \epsilon_t$$
\item Series temporales mixtas:
$$ x_t = T_t \cdot S_t + \epsilon_t$$
$$ x_t = (T_t + S_t) \cdot \epsilon_t$$
\end{itemize}

\vspace*{6mm}
**Ejemplo 4**: Encuesta de ocupaci?n hotelera. Total nacional de viajeros residentes en Espa?a. 

\vspace*{2mm}
```{r, out.width = "0.8\\textwidth"}
comp <- decompose(Viajeros_ts, type="additive")
plot(comp)
```

## Medidas de ajuste

En las secciones siguientes se presentan distintas metodolog?as para el an?lisis de una serie temporal. Ser? fundamental disponer de medidas que valoren la bondad del ajuste pues, de este modo, podremos comparar el funcionamiento de los distintos m?todos propuestos y seleccionar "el mejor" para obtener predicciones para instantes temporales futuros. 

Algunas de las medidas de ajuste m?s utilizadas son:
$$\begin{array}{lll}
\textrm{RMSE} & = & \sqrt{\frac{1}{n}\sum_{t=1}^{n}(x_t - \hat{x}_t)^2}\\
\textrm{MAD} & = & \frac{1}{n} \sum_{t=1}^{n} |x_t - \hat{x}_t|\\
\textrm{MAPE} & = & \frac{100}{n} \sum_{t=1}^{n} \frac{|x_t - \hat{x}_t|}{x_t}
\end{array}$$

donde $\hat{x}_t$ es la estimaci?n obtenida para $x_t$ con el m?todo propuesto. 

## Descomposición de una serire: método de medias móviles

```{r}

autoplot(Nile) + xlab("Year") +
  ggtitle("Annual river flow at Ashwan")
```
```{r}
autoplot(Nile, series="Data") +
  autolayer(ma(Nile,5), series="5-MA") +
  xlab("Year") + ylab("l") +
  ggtitle("Annual river flow at Ashwan") +
  scale_colour_manual(values=c("Data"="grey50","5-MA"="red"),
                      breaks=c("Data","5-MA"))
```
```{r}
opar<-par(no.readonly=TRUE)
par(mfrow=c(2,2))
ylim<-c(min(Nile),max(Nile))
plot(Nile,main="Raw time series")
plot(ma(Nile,3),main="simple moving averages (k=3)", ylim=ylim)
plot(ma(Nile,3),main="simple moving averages (k=7)", ylim=ylim)
plot(ma(Nile,3),main="simple moving averages (k=15)", ylim=ylim)
```



\newpage

# M?todos de suavizado exponencial

Los m?todos de suavizado exponencial, debido a que son relativamente sencillos de interpretar e implementar y producen resultados satisfactorios en la mayor?a de casos pr?cticos, son ampliamente utilizados en las empresas. Se tratan de modelos param?tricos deterministas que se ajustan a la evoluci?n de la serie con el objetivo de predecir valores en instantes temporales futuros. Una caracter?stica de estos m?todos es que las observaciones m?s recientes tienen m?s peso en la predicci?n que las m?s alejadas en el tiempo.    

## Suavizado exponencial simple

Los m?todos de suavizado exponencial simple fueron propuestos por C.C. Holt en 1957 para su aplicaci?n en series temporales sin tendencia ni estacionalidad. 

El pron?stico de cada observaci?n es una media ponderada entre el dato previo y su predicci?n:
$$\hat{x}_{t+1} = \alpha \cdot x_{t} + (1 - \alpha) \cdot \hat{x}_{t}, \qquad t = 1,\ldots,n-1$$
siendo $\alpha \in [0,1]$ el \textcolor{blue}{par?metro de suavizado}. 

Sea $x_0$ el valor de la \textcolor{blue}{condici?n inicial}. Aplicando el m?todo de manera recursiva, podemos ver como valores m?s cercanos en el tiempo tienen una mayor ponderaci?n:
$$\begin{array}{lll}
\hat{x}_1 & = & x_0 \\
\hat{x}_2 & = & \alpha \cdot x_1 + (1 - \alpha) \cdot \hat{x}_1  
              = \alpha \cdot x_1 + (1 - \alpha) \cdot x_0\\
\hat{x}_3 & = & \alpha \cdot x_2 + (1 - \alpha) \cdot \hat{x}_2
              = \alpha \cdot x_2 + \alpha (1 - \alpha) \cdot x_1 + (1 - \alpha)^2 \cdot x_0\\
\hat{x}_4 & = & \alpha \cdot x_3 + (1 - \alpha) \cdot \hat{x}_3
              = \alpha \cdot x_3 + \alpha (1 - \alpha) \cdot x_2 + \alpha (1 - \alpha)^2 \cdot x_1 + (1 - \alpha)^3 \cdot x_0\\
              & \vdots & \\
\hat{x}_n & = & \alpha \cdot x_{n-1} + (1 - \alpha) \cdot \hat{x}_{n-1}
              = \alpha \cdot x_{n-1} + \alpha (1 - \alpha) \cdot x_{n-2} + \ldots + (1 - \alpha)^{n-1} \cdot x_0              
\end{array}$$

Hist?ricamente, el valor de la condici?n inicial se fijaba a partir de las primeras observaciones de la serie (por ejemplo, $x_0 = x_1$) y el valor de $\alpha$ se eleg?a de manera que se minimizaran los errores de predicci?n de un paso: $\textrm{min}_\alpha \quad \frac{1}{(n-1)} \sum_{t=2}^{n}(x_t - \hat{x}_t)^{2}$. 

El procedimiento anterior supone que el valor de la condici?n inicial influye muy poco en los resultados finales. En general, especialmente si la serie es corta, es aconsejable estimar tanto el valor de $x_0$ como el de $\alpha$: $\textrm{min}_{x_0,\alpha} \quad \frac{1}{n} \sum_{t=1}^{n}(x_t - \hat{x}_t)^{2}$.

En ocasiones, se utiliza el t?rmino \textcolor{blue}{nivel} para denotar a: 
$$L_t = \alpha \cdot x_{t} + (1 - \alpha) \cdot \hat{x}_{t} = \alpha \cdot x_{t} + (1 - \alpha) \cdot L_{t-1}$$
As? pues, podemos entender que el m?todo de suavizado exponencial simple actualiza el nivel de la serie de manera recursiva conforme se incorpora una nueva observaci?n, y el nivel en el instante $t$ es utilizado como pron?stico para la observaci?n en $t+1$.

Una vez estimados $x_0$ y $\alpha$ a partir de las observaciones que componen la serie, las predicciones para instantes temporales futuros vienen dadas por:
$$\hat{x}_{n + k} = L_n, \quad k = 1,2,\ldots,h$$
siendo $h$ el horizonte de predicci?n.

## Suavizado exponencia doble: Holt

El m?todo de Holt es una extensi?n del suavizado exponencial simple que permite el manejo de tendencia. El pron?stico de cada observaci?n viene dado por la suma del \textcolor{blue}{nivel} y la \textcolor{blue}{tendencia}:
$$\hat{x}_t = L_{t-1} + T_{t-1}$$
Las siguientes ecuaciones de actualizaci?n nos permiten actualizar el nivel y la tendencia de la serie en cada periodo:
$$\begin{array}{lll}
L_t & = &\alpha \cdot x_t + (1 - \alpha) \cdot (L_{t-1} + T_{t-1})\\
T_t & = & \beta \cdot (L_t - L_{t-1}) + (1 - \beta) \cdot T_{t-1}
\end{array}$$
siendo $\alpha \in [0,1]$ y $\beta \in [0,1]$ los \textcolor{blue}{par?metros de suavizado} y $L_0$ y $T_0$ las \textcolor{blue}{condiciones iniciales}.

Una vez estimados los par?metros del modelo a partir de las observaciones que componen la serie, las predicciones para instantes temporales futuros vienen dadas por:
$$\hat{x}_{n + k} = L_n + k \cdot T_n, \quad k = 1,2,\ldots,h$$
siendo $h$ el horizonte de predicci?n.

\vspace*{2mm}
**Ejemplo 2**: N?mero de personas en activo (en miles) en la provincia de Valencia.

\vspace*{2mm}
```{r}
insample <- ts(Activos_ts[1:64],start=c(2002,1),frequency=4)  
      # ajuste desde (2002,1) hasta (2017,4)
outsample <- ts(Activos_ts[65:68],start=c(2018,1),frequency=4) 
      # utilizamos 2018 para valorar predicci?n

fitActivos <- holt(insample,h=4)
fitActivos$model
fitval <- fitActivos$fitted
plot(fitval,col="blue",ylab="Num de activos")
lines(insample)
rmse <- sqrt(mean((insample-fitval)^2))
rmse
mape <- 100*mean(abs(insample-fitval)/insample)
mape

fitActivos
plot(fitActivos)
pred <- fitActivos$mean
rmse_pred <- sqrt(mean((outsample-pred)^2))
rmse_pred
mape_pred <- 100*mean(abs(outsample-pred)/outsample)
mape_pred
``` 
## Suavizado exponencial triple: Holt-Winters 

El modelo de Holt fue generalizado por Winters en 1965 para incluir estacionalidad. El pron?stico de cada observaci?n viene dado por la suma del \textcolor{blue}{nivel}, la \textcolor{blue}{tendencia} y la \textcolor{blue}{estacionalidad}. 

En el caso de \textcolor{blue}{estacionalidad aditiva}:
$$\hat{x}_t = L_{t-1} + T_{t-1} + S_{t-c}$$
Las ecuaciones de actualizaci?n en este caso vienen dadas por:
$$\begin{array}{lll}
L_t & = & \alpha \cdot (x_t - S_{t-c}) + (1 - \alpha) \cdot (L_{t-1} + T_{t-1})\\
T_t & = & \beta \cdot (L_t - L_{t-1}) + (1 - \beta) \cdot T_{t-1}\\
S_t & = & \gamma \cdot (x_t - L_t) + (1 - \gamma) \cdot S_{t-c}
\end{array}$$

siendo $\alpha \in [0,1]$, $\beta \in [0,1]$ y $\gamma \in [0,1]$ los \textcolor{blue}{par?metros de suavizado} y $L_0$, $T_0$ y $(S_{1-c},S_{2-c},\ldots,S_{0})$ las \textcolor{blue}{condiciones iniciales}. 

Si se supone \textcolor{blue}{estacionalidad multiplicativa}: 
$$\hat{x}_t = (L_{t-1} + T_{t-1}) \cdot S_{t-c}$$
y las ecuaciones de actualizaci?n correspondientes son:
$$\begin{array}{lll}
L_t & = & \alpha \cdot (x_t / S_{t-c}) + (1 - \alpha) \cdot (L_{t-1} + T_{t-1})\\
T_t & = & \beta \cdot (L_t - L_{t-1}) + (1 - \beta) \cdot T_{t-1}\\
S_t & = & \gamma \cdot (x_t / L_t) + (1 - \gamma) \cdot S_{t-c}
\end{array}$$

Una vez estimados los par?metros del modelo a partir de las observaciones que componen la serie, las predicciones para instantes temporales futuros vienen dadas por:
$$\begin{array}{lll}
\hat{x}_{n + k} & = & L_n + k \cdot T_n + S_{n+k-c} \quad (\textrm{caso aditivo})\\
\hat{x}_{n + k} & = & (L_n + k \cdot T_n) \cdot S_{n+k-c} \quad (\textrm{caso multiplicativo})
\end{array}$$
para $k = 1,2,\ldots,h=c$. 


\vspace*{5mm}
Una alternativa al m?todo multiplicativo consiste en realizar previamente una transformaci?n logar?tmica a los datos. La serie logar?tmica se analiza con el m?todo de Holt-Winters aditivo. A las predicciones obtenidas se les aplica una transformaci?n exponencial para regresar a las unidades originales. 


\vspace*{4mm}
**Ejemplo 4**: Encuesta de ocupaci?n hotelera. Total nacional de viajeros residentes en Espa?a. 

\vspace*{2mm}
```{r}
insample <- ts(Viajeros_ts[1:96],start=c(2010,1),frequency=12)  
      # ajuste desde (2010,1) hasta (2017,12)
outsample <- ts(Viajeros_ts[97:108],start=c(2018,1),frequency=12) 
      # utilizamos 2018 para valorar predicci?n

fitViajeros <- hw(insample,h=12,seasonal="additive")
fitViajeros$model
fitval <- fitViajeros$fitted
plot(fitval,col="blue",ylab="Viajeros residentes en Espa?a")
lines(insample)
rmse <- sqrt(mean((insample-fitval)^2))
rmse
mape <- 100*mean(abs(insample-fitval)/insample)
mape
fitViajeros
plot(fitViajeros)
pred <- fitViajeros$mean
rmse_pred <- sqrt(mean((outsample-pred)^2))
rmse_pred
mape_pred <- 100*mean(abs(outsample-pred)/outsample)
mape_pred
```

\vspace*{6mm}

**Ejercicio:** ?Se mejora el ajuste y la predicci?n de la serie que recoge el n?mero de personas en activo en la provincia de Valencia si se utiliza el m?todo de Holt-Winters?

**Ejercicio:** Analiza la serie correspondiente al n?mero de usuarios conectados a internet por minuto. Utiliza los ?ltimos 5 datos para valorar la capacidad predictiva del m?todo utilizado.

**Ejercicio:** Analiza la serie correspondiente al tr?fico comercial en el aeropuerto de Valencia. Utiliza los datos del a?o 2019 para valorar la capacidad predictiva del m?todo utilizado.


\vspace*{6mm}
\sffamily\fboxrule.1em\fboxsep1em
\fcolorbox{black}{white}{\color{blue}
\begin{minipage}[c][1cm][t]{15cm}
La funci?n ets del paquete forecast es muy interesante para el an?lisis de series temporales con suavizado exponencial. Las transformaciones Box-Cox incluidas nos permiten obtener otras transformaciones alternativas al logaritmo.
\end{minipage}
}

\newpage

# Metodolog?a Box-Jenkins

La metodolog?a Box-Jenkins se desarroll? en 1970 con el objetivo de ajustar un tipo especial de modelos, denominados ARIMA (modelos autorregresivos integrados de medias m?viles), a una serie temporal. La idea de estos modelos es describir el valor observado en un periodo $t$ como una funci?n lineal de valores anteriores y errores debidos al azar. Una vez ajustado el modelo, se puede predecir la evoluci?n de la serie en el futuro. 

En la especificaci?n de estos modelos entran distintos tipos de par?metros que capturan distintos rasgos de los datos. Las etapas que componen el ajuste de un modelo ARIMA son:
\begin{itemize}
\item Determinaci?n de la transformaci?n estacionaria de la serie (transformaci?n de los datos originales, por ejemplo, la serie de los logaritmos, diferencias regulares, diferencias estacionales). 
\item Identificaci?n de un modelo basado en el an?lisis de las autocorrelaciones y autocorrelaciones parciales. 
\item Estimaci?n de los par?metros del modelo identificado.
\item An?lisis de los residuos para verificar el modelo ajustado (hay que garantizar que la serie de los residuos tiene estructura de ruido blanco).
\end{itemize}

**Conceptos b?sicos**

En la secci?n 1.2 hemos introducido el concepto de covarianza entre dos variables aleatorias como:
$$
\gamma(t,s) = E[(X_t - \mu_t)(X_s - \mu_s)].
$$
A partir de esta covarianza, se define la correlaci?n como:
$$
\rho(t,s) = \frac{\gamma(t,s)}{\sigma_t \sigma_s}.
$$
Para series estables en covarianza ($\gamma(t,t+h) = \gamma_h$), se define la \textcolor{blue}{funci?n de autocorrelaci?n simple (ACF)} a la funci?n que proporciona las correlaciones en funci?n del retardo $h$ y, por tanto, proporciona informaci?n sobre la estructura de dependencia lineal de la serie. Si $x_t \rightarrow x_{t+1}$ indica que $x_t$ influye sobre $x_{t+1}$, la funci?n de autocorrelaci?n simple tiene por objetivo estudiar c?mo una observaci?n influye en las posteriores:
$$
x_1 \rightarrow x_2 \rightarrow x_3 \rightarrow \ldots \rightarrow x_{t} \rightarrow x_{t+1} \rightarrow \ldots   
$$
La funci?n de autocorrelaci?n simple tiene las siguientes propiedades: $\rho(0) = 1$, $-1 \leq \rho(h) \leq 1$. Una correlaci?n positiva indica que valores grandes en un instante concreto se corresponden con valores grandes en el retardo especificado. Por el contrario, una correlaci?n negativa indica que valores grandes en un instante concreto se corresponden con valores peque?os en el retardo especificado.

El \textcolor{blue}{correlograma} o gr?fico de autocorrelaci?n es una representaci?n gr?fica de las autocorrelaciones de la muestra.

La \textcolor{blue}{funci?n de autocorrelaci?n parcial (PACF)} mide la correlaci?n entre dos variables separadas $h$ periodos cuando no se considera la dependencia creada por los retardos intermedios existentes entre ambas.

El \textcolor{blue}{correlograma parcial} es una representaci?n gr?fica de las autocorrelaciones parciales de la muestra.

\vspace*{6mm}

**Ejemplo 1**: N?mero de usuarios conectados a internet a trav?s de un servidor por minuto.

\vspace*{2mm}
```{r}
acf(WWWusage,lag.max=50)
pacf(WWWusage,lag.max=50)
```

\vspace*{6mm}
**Ejemplo 4**: Encuesta de ocupaci?n hotelera. Total nacional de viajeros residentes en Espa?a. 

\vspace*{2mm}
```{r}
acf(Viajeros_ts,lag.max=50)
pacf(Viajeros_ts,lag.max=50)
```

## Modelos ARMA

Se dice que un \textcolor{blue}{proceso} es \textcolor{blue}{autorregresivo} cuando la observaci?n en un instante $t$ depende linealmente de las observaciones en periodos anteriores m?s un t?rmino de error. 

\textcolor{blue}{Modelo AR(p)}: 

Los modelos autorregresivos se conocen como AR(p), donde $p$ indica el orden del modelo; es decir, la cantidad de observaciones pasadas que influyen en la observaci?n en el instante $t$. Estos modelos quedan determinados por la siguiente ecuaci?n:
$$
x_t = \phi_0 + \phi_1 \cdot x_{t-1} + \phi_2 \cdot x_{t-2} + \ldots + \phi_p \cdot x_{t-p} + \epsilon_t
$$
donde $\epsilon_t \sim N(0,\sigma^{2})$ es el t?rmino de error, que permite que la serie no sea determinista.

Los procesos autorregresivos no son siempre estacionarios. Un proceso AR(p) es estacionario si y s?lo si el m?dulo de las ra?ces del polinomio autorregresivo, definido como:
$$
1 - \phi_1 B - \phi_2 B^{2} - \ldots...- \phi_p B^{p} = 0,
$$
est? fuera del c?rculo unidad. 

La funci?n de autocorrelaci?n simple de un modelo AR(p) decrece lentamente. La funci?n de autocorrelaci?n parcial verifica que las $p$ primeras autocorrelaciones son distintas de 0 y el resto son 0 (punto de corte en $p$). 

El proceso autorregresivo m?s simple es el AR(1): $x_t = \phi_0 + \phi_1 x_{t-1} + \epsilon_t$. La condici?n de estacionariedad en este caso es $|\phi_1| < 1$. Algunas propiedades de este modelo son:
\begin{itemize}
\item $E(X_t) = \phi_0/(1-\phi_1)$
\item $V(X_t) = \sigma^{2}/(1 - \phi_1^{2})$
\item $\rho(h) = \phi_1^{h}$ (decrecimiento exponencial).
\end{itemize}

La funci?n de autocorrelaci?n de un modelo AR(1) decrece a 0 en valor absoluto. El decrecimiento depende de $|\phi_1|$: cuanto mayor es, m?s lento es el decrecimiento. La funci?n de autocorrelaci?n parcial s?lo tendr? el primer coeficiente significativo, pues s?lo existe influencia de primer orden (si $x_t$ depende de $x_{t-2}$ es a trav?s de $x_{t-1}$). 

\vspace{2mm}
El \textcolor{blue}{modelo de medias m?viles} se utiliza para describir aquellas situaciones en las que el valor de la variable en un periodo $t$ depende de una sucesi?n de errores correspondientes a periodos anteriores. A diferencia de los procesos autorregresivos, estos procesos tienen memoria corta, pues absorben r?pidamente los impactos.  

\textcolor{blue}{Modelo MA(q)}: 

Los modelos de medias m?viles se representan por MA(q), donde $q$ indica el orden del modelo. Si suponemos media 0, la formulaci?n de estos modelos es como sigue:
$$
x_t = \epsilon_t - \theta_1 \cdot \epsilon_{t-1} - \theta_2 \cdot \epsilon_{t-2} - \ldots - \theta_q \cdot \epsilon_{t-q}
$$
donde $\{e_t\}$ son los t?rminos de error. Generalmente se asume que estos errores son independientes e id?nticamente distribuidos seg?n $N(0,\sigma^{2})$.

Todos los modelos de medias m?viles finitos son siempre estacionarios para cualquier valor de sus par?metros. El modelo MA(q) tiene media constante y cero, varianza constante y finita y su funci?n de autocovarianzas est? truncada a partir del retardo $q$; es decir: $\rho(h) \neq 0$ si $h \leq q$ y $\rho(h) = 0$ si $h > q$. La funci?n de autocorrelaci?n parcial decrece con el retardo de forma exponencial. 

La formulaci?n anterior puede generalizarse para representar modelos con media no nula:
$$
x_t = \mu + \epsilon_t - \theta_1 \cdot \epsilon_{t-1} - \theta_2 \cdot \epsilon_{t-2} - \ldots - \theta_q \cdot \epsilon_{t-q}
$$
El modelo de medias m?viles m?s sencillo es el MA(1): $x_t = \epsilon_t - \theta_1 \cdot \epsilon_{t-1}$, que determina el valor de $x_t$ en funci?n del error contempor?neo y su primer retardo. Su funci?n de autocorrelaci?n se trunca en el primer retardo.  

\vspace{2mm}
Los \textcolor{blue}{procesos autorregresivos de medias m?viles} determinan el valor de $x_t$ en funci?n de su pasado hasta el retardo $p$, del error contempor?neo y la sucesi?n de errores en periodos anteriores hasta el retardo $q$. 

\textcolor{blue}{Modelo ARMA(p,q)}: 

Est? formado por dos partes, una parte autorregresiva (AR) y otra de media m?vil (MA), siendo $p$ el orden de la parte autorregresiva y $q$ el de la parte de media m?vil. La formulaci?n viene dada por:
$$
x_t = \phi_0 + \phi_1 \cdot x_{t-1} + \phi_2 \cdot x_{t-2} + \ldots + \phi_p \cdot x_{t-p} + \epsilon_t - \theta_1 \cdot \epsilon_{t-1} - \theta_2 \cdot \epsilon_{t-2} - \ldots - \theta_q \cdot \epsilon_{t-q}
$$
O, equivalentemente:
$$
(1 - \phi_1 B - \phi_2 B^{2} - \ldots - \phi_p B^{p}) x_t = \phi_0 + (1 - \theta_1 B - \theta_2 B^{2} - \ldots - \theta_q B^{q}) \epsilon_t,
$$
donde $B$ es el operador retardo tal que: $B x_{t} = x_{t-1}$, $B^{2} x_{t} = x_{t-2}$ y as? sucesivamente.

El modelo ARMA(p,q) comparte caracter?sticas de los modelos AR(p) y MA(q) ya que contiene ambas estructuras a la vez. A partir del retardo $q$, la funci?n de autocorrelaci?n decrece r?pidamente hacia cero siguiendo la estructura marcada por la parte AR. Respecto a la funci?n de autocorrelaci?n parcial, los $p$ primeros coeficientes vendr?n establecidos por la parte AR. A partir del retardo $p$ se producir? un decrecimiento que vendr? marcado por la estructura MA. 

## Modelos ARIMA

Los modelos ARMA ?nicamente sirven para ajustar series estacionarias. As? pues, el primer paso es transformar la serie temporal en una serie estacionaria. Si la serie temporal presenta tendencia, podemos trabajar con la serie diferenciada una vez:
$$
y_t = x_t - x_{t-1}, \quad t = 2, 3, 4,\ldots,n
$$
o con la serie resultante de aplicar dos diferencias:
$$
y'_t = y_t - y_{t-1} = x_{t} - 2 x_{t-1} + x_{t-2}, \quad t = 3, 4,\ldots,n
$$

Denotamos por $d$ al orden de integraci?n de la serie; es decir, el n?mero de diferencias que se deben aplicar para convertir la serie en estacionaria. Normalmente, valores de $d = 1,2$ son suficientes. Un \textcolor{blue}{modelo ARIMA(p,d,q)} es el resultado de ajustar un modelo ARMA(p,q) a la serie resultante de aplicar $d$ diferencias. 

Si denotamos por $\bigtriangledown x_t = (1 - B) x_t = x_t - x_{t-1}$ al operador diferencia, el modelo ARIMA(p,d,q) puede representarse como:
$$
(1 - \phi_1 B - \phi_2 B^{2} - \ldots - \phi_p B^{p}) \bigtriangledown^{d} x_t = \phi_0 + (1 - \theta_1 B - \theta_2 B^{2} - \ldots - \theta_q B^{q}) \epsilon_t
$$

Una vez identificada la serie, es decir cuando se conoce en modelo ARIMA(p,d,q) que puede describir la serie, se pasa a la estimaci?n de los par?metros. Esta estimaci?n requiere de algoritmos de optimizaci?n complejos. Si la serie est? bien ajustada, los residuos deben ser ruido blanco. 

**Ejemplo 1**: N?mero de usuarios conectados a internet a trav?s de un servidor por minuto.

\vspace*{2mm}
```{r, out.width = "0.5\\textwidth"}
dusers <- diff(WWWusage) # Serie diferenciada una vez
plot(dusers,type="l") # Gr?fico de la serie diferenciada

acf(dusers,lag.max=50)
pacf(dusers,lag.max=50)
```

A partir del correlograma y correlograma parcial, podr?amos pensar que la serie diferenciada sigue un ARMA(1,1); es decir, la serie temporal puede ser descrita mediante un modelo ARIMA(1,1,1):

```{r}
wwwusage_model <- arima(WWWusage,order=c(1,1,1))
accuracy(wwwusage_model)
checkresiduals(wwwusage_model,plot=TRUE)
forecast(wwwusage_model,h=5)
plot(forecast(wwwusage_model,h=5))
```

## Modelos sARIMA

Muchas series presentan estacionalidad, de manera que la serie presenta una pauta que se repite a?o tras a?o. El orden de la estacionalidad indica cada cu?ntos periodos se repite el ciclo. Las series estacionales presentan la estructura de dependencia de las series no estacionales (estructura regular) m?s una estructura adicional (estructura estacional).

El ajuste de modelos a series estacionales requiere, como paso previo, eliminar el ciclo estacional. Esto se consigue tomando diferencias de orden estacional:
$$
\bigtriangledown_c \, x_t =  (1 - B^c) x_t = x_t - x_{t-c}.
$$
El an?lisis de la ACF y PACF en series estacionales previamente diferenciadas nos permite obtener informaci?n sobre:
\begin{itemize}
\item Parte regular, que se identifica a partir de los primeros retardos. 
\item Parte estacional, que puede identificarse estudiando la funci?n de autocorrelaci?n y de autocorrelaci?n parcial en los retardos estacionales ($c$,$2c$,$3c,\ldots,$).
\item Alrededor de los retardos estacionales, observamos la interacci?n entre la parte estacional y regular; es decir, se observa la repetici?n de la funci?n de autocorrelaci?n de la parte regular a ambos lados de los retardos estacionales. 
\end{itemize}

Un \textcolor{blue}{modelo sARIMA(p,d,q)(P,D,Q)} es el resultado de ajustar un modelo ARMA generalizado a la serie resultante de aplicar $d$ diferencias regulares y $D$ diferencias estacionales, que incluya adem?s de la dependencia regular tambi?n la estacional. El modelo sARIMA(p,d,q)(P,D,Q) puede representarse como:
$$\begin{array}{l}
(1 - \phi_1 B - \phi_2 B^{2} - \ldots - \phi_p B^{p}) (1 - \Phi_1 B^c - \Phi_2 B^{2c} - \ldots - \Phi_P B^{Pc}) \bigtriangledown^{d} \bigtriangledown_{c}^{D} x_t = \\
\hspace*{2mm}\phi_0 + (1 - \theta_1 B - \theta_2 B^{2} - \ldots - \theta_q B^{q}) (1 - \Theta_1 B^c - \Theta_2 B^{2c} - \ldots - \Theta_Q B^{Qc})\epsilon_t
\end{array}$$


\vspace*{6mm}
**Ejemplo 4**: Encuesta de ocupaci?n hotelera. Total nacional de viajeros residentes en Espa?a. 

\vspace*{2mm}
```{r}
insample <- ts(Viajeros_ts[1:96],start=c(2010,1),frequency=12)  
      # ajuste desde (2010,1) hasta (2017,12)
outsample <- ts(Viajeros_ts[97:108],start=c(2018,1),frequency=12) 
      # utilizamos 2018 para valorar predicci?n

dViajeros <- diff(insample)
plot(dViajeros,type="l")
```
Hemos quitado la tendencia con la diferencia regular, pero a?n queda la estacionalidad.

```{r}
ddcViajeros <- diff(dViajeros,12)
plot(ddcViajeros,type="l")
```
La serie diferenciada con $d=1$ y $D=1$ ya es estacionaria. Pasamos pues a examinar el correlograma y correlograma parcial:
```{r}
acf(ddcViajeros,lag.max=50)
pacf(ddcViajeros,lag.max=50)
```

\vspace*{6mm}
\sffamily\fboxrule.1em\fboxsep1em
\fcolorbox{black}{white}{\color{blue}
\begin{minipage}[c][1cm][t]{15cm}
La funci?n auto.arima del paquete forecast es muy interesante para el an?lisis de series temporales mediante la metodolog?a Box-Jenkins.
\end{minipage}
}

\vspace*{5mm}
```{r}
Viajeros_model <- auto.arima(insample)
Viajeros_model
accuracy(Viajeros_model)
checkresiduals(Viajeros_model,plot=TRUE)
pred <- forecast(Viajeros_model,h=12)$mean
pred
plot(forecast(Viajeros_model,h=12))

rmse_pred <- sqrt(mean((outsample-pred)^2))
rmse_pred
mape_pred <- 100*mean(abs(outsample-pred)/outsample)
mape_pred
```


\vspace*{6mm}
**Ejercicio:** Analiza la serie correspondiente al tr?fico comercial en el aeropuerto de Valencia. Utiliza los datos del a?o 2019 para valorar la capacidad predictiva del modelo utilizado.


