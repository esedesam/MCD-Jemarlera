---
title: "Practica_2"
author: "Jesús Martínez Leal"
date: "2023-11-02"
output: html_document
---

```{r}
library(ggplot2)
set.seed(33)
```


Cargue los datos en el archivo **angles.txt**; los datos han sido generados a partir de una densidad $f(x|k) ∝ sin(x)^k$ en el intervalo $[0, π]$ $(f(x|k) = 0$ fuera del intervalo $[0, π]$). Notación: ∝ significa *proporcional a*, lo que significa que $f(x|k) = C_ksin(x)^k$, donde $C_k$ es una constante de normalización apropiada.

```{r}
angles <- readLines(con = "./data/angles.txt")
angles <- angles[-1]
a <- unlist(strsplit(angles, split = " "))
a <- matrix(a, ncol = 2 ,byrow = T)
a <- a[, 2]
a <- as.numeric(a)
```


# Ejercicio 1

## Ejercicio 1.0 
¿Cómo puede calcular la constante de normalización $C_k?$?

Para calcular la constante de normalización $C_k$ el método de los momentos.

$$\mathbb{E}(X) = \int_{0}^\pi xC_ksin(x)^k dx$$
## Ejercicio 1.1 ¿El modelo es paramétrico? ¿Cuáles son los parámetros del modelo?

El modelo es paramétrico porque se puede parametrizar con un número finito de variables. En este caso el parámetros es **k**.

## Ejercicio 1.2

Escriba la función del logaritmo negativo de la verosimilitud del modelo e impleméntela en una función de R.

La función de verosimilitud es:

$$ \mathbb{L}_n(k) = \prod_{i = 1}^n  C_ksin(x)^k $$

Y por tanto, la función del logaritmo negativo de la verosimilitud queda de la forma:

$$l_n(k) = -\sum_{i = 1}^n \left(ln(C_k) + k·ln(sin(x))\right)$$

Y se implementaría en R como sigue:

```{r}
C_norm <- function(k, obs) sin(obs)**k


l_n <- function(k, obs) -sum(log(1/integrate(C_norm, lower = 0, upper = 3.14, k = k)$value) + k*log(sin(obs)))



```

## Ejercicio 1.3

Utilice un método de optimización numérica para encontrar el estimador de máxima verosimilitud


```{r}
k_opt <- optimize(l_n, obs = a, lower = 5, upper = 20)
```
Vemos que el valor para el estimador es k = 11.4

## Ejercicio 1.4

Grafique el histograma de los datos y la densidad correspondiente al estimador de máxima verosimilitud.

```{r}
df_a <- data.frame(a)

ggplot(df_a, aes(a)) +
  geom_histogram(aes(y = after_stat(density)), colour = "black", fill = "white") +
  geom_density(alpha = .2, fill="red") +
  labs(title = "Histogram and Density Plot of Maximum Likelihood Estimator", x = "Data", y = "Density")

```

## Ejercicio 1.5

Construye un intervalo de confianza del 99% para k basado en los datos en el archivo angles.txt. ¿Cómo puedes estimar el error estándar?

```{r}
#Método de bootstrap para sacar distribución de k
v_sample <- replicate(1000, {
  aux <- sample(df_a$a, size = 1000, replace = T)
  kest <- optimize(l_n, obs = aux, lower = 5, upper = 20)$minimum
  
  return(kest)
})

#Intervalo normal
k_std <- sd(v_sample)
alpha <- 0.01
za2 <- qnorm(alpha / 2, lower.tail = F)
k_trust <- k_opt$minimum + c(-1,1)*za2*k_std

#Intervalo de percentil
k_perct <- quantile(v_sample, probs = c(alpha/2, 1-alpha/2))

```

## Ejercicio 1.6

Prueba si k > 10 a un nivel de confianza α = 0.05 para los datos en el archivo angles.txt (puedes utilizar la prueba de Wald H0 : k ≤ 10).

```{r}
k_0 <- 10
w <- (k_opt$minimum - k_0)/k_std
p_value <- 2*pnorm(-abs(w))

cat("Dado que el p-value de la prueba de Wald es", p_value, "podemos rechazar la hipótesis nula (H0 : k ≤ 10)")


```

