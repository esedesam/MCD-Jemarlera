{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 1. Optimización restringida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "from scipy.optimize import least_squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    " Dada la función $f(x) = x^2e^{−x^2}$, represéntala gráficamente y determina su mínimo en el intervalo [−3,2]\n",
    " mediante el método de la bisección. El criterio de convergencia es que la amplitud del intervalo sea menor\n",
    " que 0.001. Utiliza también el método de Newton, tomando como valores iniciales 2, 1.2, 0.5. El criterio de\n",
    " convergencia en este caso es que el valor absoluto de la primera derivada sea menor de 0.0001. Comenta lo\n",
    " que observas y el motivo de ello. ¿Qué alternativa propondrías?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2*np.exp(-x**2)\n",
    "\n",
    "def f_prime(x):\n",
    "    return -2*x*(x**2-1)*np.exp(-x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El máximo aproximado es -0.9999923706054688\n",
      "El mínimo aproximado es 0.0\n"
     ]
    }
   ],
   "source": [
    "def bisect_minimum(derivative, a, b, tol=0.0001, max_iter=100):\n",
    "\n",
    "    if derivative(a) * derivative(b) >= 0:\n",
    "        raise ValueError(\"Las derivadas en los extremos deben tener signos distintos.\")\n",
    "\n",
    "    iteration = 0\n",
    "\n",
    "    while (b - a) > tol and iteration < max_iter:\n",
    "        c = (a + b) / 2\n",
    "        if derivative(c) == 0:\n",
    "            return c\n",
    "        elif derivative(c) * derivative(a) < 0:\n",
    "            b = c\n",
    "        else:\n",
    "            a = c\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return (a + b) / 2\n",
    "\n",
    "# Ejemplo pedido:\n",
    "minimum = bisect_minimum(f_prime, -3, 2)\n",
    "print(\"El máximo aproximado es\", minimum) #Con este intervalo incial, la función estaría localizando el máximo localizado en x = -1\n",
    "\n",
    "minimum = bisect_minimum(f_prime, -0.9, 0.9)\n",
    "print(\"El mínimo aproximado es\", minimum) #Al cambiar el intervalo de búsqueda, la función converge al mínimo en x ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mínimo de la función partiendo de x = 2 se encuentra en x= 10.5117468365114 . El algoritmo no converge.\n",
      "El punto de inflexión comenzando en x = 1.2 es: \n",
      "Maximum found at x = 0.999999999999878\n",
      "El mínimo de la función partiendo de x = 0.5 se encuentra en x= 10.7387312892320 . El algoritmo no converge.\n"
     ]
    }
   ],
   "source": [
    "f2 = 'x**2*exp(-x**2)'\n",
    "\n",
    "\n",
    "\n",
    "def newton_optimization_1d(f, x0, tol = 1e-6, max_iter = 100):\n",
    "    \n",
    "    x = sp.symbols('x')\n",
    "    \n",
    "    f_expr = sp.sympify(f)\n",
    "    \n",
    "    f_prime = f_expr.diff(x)\n",
    "    f_double_prime = f_prime.diff(x)\n",
    "    \n",
    "    x_current = x0\n",
    "    iteration = 0\n",
    "    \n",
    "    while iteration < max_iter:\n",
    "        df_prime = f_prime.subs(x, x_current)\n",
    "        df_double_prime = f_double_prime.subs(x, x_current)\n",
    "        \n",
    "        x_new = x_current - df_prime / df_double_prime # iteración\n",
    "        \n",
    "        if abs(x_new - x_current) < tol: # salida por tolerancia\n",
    "            \n",
    "            second_derivative = df_double_prime.subs(x, x_new)\n",
    "            \n",
    "            if second_derivative > 0:\n",
    "                print(f\"Minimum found at x = {x_new}\")\n",
    "            elif second_derivative < 0:\n",
    "                print(f\"Maximum found at x = {x_new}\")\n",
    "            else:\n",
    "                print(f\"Saddle point found at x = {x_new}\")\n",
    "                \n",
    "            return x_new\n",
    "        \n",
    "        x_current = x_new\n",
    "        iteration += 1\n",
    "        \n",
    "        \n",
    "    return x_current\n",
    "\n",
    "minimo2 = newton_optimization_1d(f2, 2.0, tol = 0.0001)\n",
    "print(\"El mínimo de la función partiendo de x = 2 se encuentra en x=\", minimo2, \". El algoritmo no converge.\")\n",
    "\n",
    "#Como podemos observar, al comenzar en x = 2, la función no es capaz de converger a ningún punto\n",
    "# de inflexión. Es más, si aumentamos el número de iteraciones la supuesta posición del mínimo \n",
    "# tiende a números cada vez más grandes\n",
    "\n",
    "print(\"El punto de inflexión comenzando en x = 1.2 es: \")\n",
    "minimo1_2 = newton_optimization_1d(f2, 1.2, tol = 0.0001)\n",
    "\n",
    "# Al comenzar en x = 1.2 el algoritmo se encuentra muy cerca del máximo local en x = 1, por lo tanto\n",
    "# converge a este punto. Al contrario que los otros dos puntos de partida, en este caso el algoritmo sí qiue converge\n",
    "\n",
    "minimo0_5 = newton_optimization_1d(f2, 0.5, tol = 0.0001)\n",
    "print(\"El mínimo de la función partiendo de x = 0.5 se encuentra en x=\", minimo0_5, \". El algoritmo no converge.\")\n",
    "\n",
    "# Al comenzar en x = 0.5 el algoritmo en la primera iteración pasa a la derecha del primer máximo, \n",
    "# ya que en ese punto tiene primera derivada negativa y segunda positiva. Una vez pasa de x = 1 y se\n",
    "# aleja, observamos el mismo comportamiento que cuando comenzamos en x = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para conseguir la convergencia al punto deseado podríamos valorar varias alternativas. En primer lugar, como ya hemos comprobado el punto inicial tiene gran relevancia a la hora de localizar los puntos relevantes de la función, un amejor selección ayudaría a la convergencia. Por otro lado, podríamos utilizar un método que, pese a necesitar más iteraciones, asegurase en mayor medida la convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    " La tabla muestra la población de Estados Unidos (en millones) en diferentes años\n",
    "\n",
    " \n",
    "|Año| Población|\n",
    "|:----:|:---------:|\n",
    " |1815| 8.3|\n",
    " |1825| 11.0|\n",
    " |1835| 14.7|\n",
    " |1845|19.7|\n",
    " |1855| 26.7|\n",
    " |1865| 35.2|\n",
    " |1875|44.4|\n",
    " |1885| 55.9|\n",
    "\n",
    " Tenemos, por tanto, un conjunto de datos $(t_j, y_j)$ donde $t_j$ es el año e $y_j$ la población. Por simplicidad,\n",
    " el año 1815 se etiqueta como t1 = 1, el año 1825 como t2 = 2, y así sucesivamente. Ajustar el modelo\n",
    " $φ(x,t) = x_1e^{x_2t}$ comenzando en x1 = 6 y x2 = 0.3, utilizando el método de Levenberg-Marquardt. El criterio\n",
    " de parada es que la variación en la suma de los cuadrados de los residuos sea menor de 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los parámetros son: [7.00009288 0.2620779 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def f(x, a, b):\n",
    "    return a * np.exp(b * x)\n",
    "\n",
    "def levenberg_marquardt(x, y, a0, b0):\n",
    "    def fun(params):\n",
    "        return f(x, *params) - y\n",
    "\n",
    "    result = least_squares(fun, [a0, b0], method='lm', ftol = 0.01)\n",
    "    return result.x\n",
    "\n",
    "# Example usage\n",
    "x1 = np.array([1815, 1825, 1835, 1845, 1855, 1865, 1875, 1885])\n",
    "x = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "y = np.array([8.3, 11.0, 14.7, 19.7, 26.7, 35.2, 44.4, 55.9])\n",
    "a0, b0 = (6, 0.3)\n",
    "params = levenberg_marquardt(x, y, a0, b0)\n",
    "print(f\"Los parámetros son: {params}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
