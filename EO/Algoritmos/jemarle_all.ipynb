{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método del gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Búsqueda inexacta en línea para el paso:\n",
    "def backtrackingline(func, gradient, xk, direction, lamb = 0.5, c1 = 10e-4, max_iter = 100): \n",
    "    \n",
    "    def armijo_condition(lamb):\n",
    "        x_new = xk + lamb * direction\n",
    "        func_value = func(*x_new) \n",
    "        return (func_value <= func(*xk) + c1 * lamb * gradient.dot(direction))\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        if armijo_condition(lamb):\n",
    "            break\n",
    "        lamb = lamb / 2  # Reducción del paso si no se cumple la condición de Armijo\n",
    "\n",
    "    return lamb \n",
    "\n",
    "# Test de convergencia de descenso lineal\n",
    "def test_convergence_linear_descend(x0, x_new, func, gradient_point_new, tol1, tol2, tol3):\n",
    "\n",
    "    conditions = (np.linalg.norm(x0 - x_new) < tol1,\n",
    "                  np.linalg.norm(gradient_point_new) < tol2,\n",
    "                  abs(func(*x_new) - func(*x0)) < tol3)\n",
    "    \n",
    "    test = any(conditions)\n",
    "    condition_idx = np.where(conditions)[0]\n",
    "\n",
    "    return test, condition_idx\n",
    "\n",
    "# Método del gradiente\n",
    "def gradient_linear_descend(f, x, x0, tol1 = 1e-4, tol2 = 1e-4, tol3 = 1e-4, max_iterations = 100):\n",
    "    \n",
    "    \n",
    "    func = sp.lambdify(x, f, 'numpy')\n",
    "    gradient = [sp.diff(f, xi) for xi in x] # expresión simbólica del gradiente\n",
    "    gradient_func = sp.lambdify(x, gradient, 'numpy')\n",
    "    \n",
    "    gradient_point = gradient_func(*x0)\n",
    "    gradient_point = np.array(gradient_point) # pasamos a numpy array para poder operar sobre él\n",
    "    d0 = -gradient_point # dirección inicial\n",
    "    \n",
    "    iteration = 0\n",
    "    results = []\n",
    "    \n",
    "    results.append([iteration, np.round(x0, 5)])\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "\n",
    "        lamb = backtrackingline(func, gradient_point, x0, d0) # obtención del paso con Backtracking LineSearch\n",
    "        \n",
    "        x_new = x0 + lamb * d0 # Aquí se obtiene el x_{k + 1}\n",
    "        gradient_point_new = np.array(gradient_func(*x_new))\n",
    "        d_new = -gradient_point_new\n",
    "        \n",
    "        test, cond_idx = test_convergence_linear_descend(x0, x_new, func, gradient_point_new, tol1, tol2, tol3)\n",
    "        \n",
    "        if test:\n",
    "            print(f\"Convergencia conseguida debido a la condición: {cond_idx}\")\n",
    "            break\n",
    "        else:\n",
    "            x0 = x_new\n",
    "            d0 = d_new\n",
    "            gradient_point = gradient_point_new\n",
    "            iteration = iteration + 1\n",
    "\n",
    "        results.append([iteration, np.round(x0, 5)])\n",
    "        \n",
    "    table = pd.DataFrame(results)\n",
    "    table.columns = ['Iteration', 'x']\n",
    "    \n",
    "    return table, x_new\n",
    "    \n",
    "# Llamada a función\n",
    "\n",
    "x, y = sp.symbols('x y')\n",
    "f = 9*x**2 + 2*x*y + y**2\n",
    "\n",
    "initial_guess = [0, -1]\n",
    "\n",
    "result, xopt = gradient_linear_descend(f, (x, y), initial_guess)\n",
    "print(result)\n",
    "\n",
    "print(f\"\\nEl valor óptimo corresponde con el x^(k + 1) de la última iteración: x* ≈ {xopt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método de Newton (multivariable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "def newtons_method(f, x, initial_guess, tol = 1e-6, max_iter = 100):\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Método de Newton para hallar mínimos\n",
    "    \n",
    "    Parámetros:\n",
    "    f: function to minimize\n",
    "    x: Lista de símbolos Sympy\n",
    "    initial_guess: Punto inicial\n",
    "    tol: tolerancia. La condición se da cuando la norma del gradiente sea menor que cierto nivel\n",
    "    max_iter: Máximo número de iteraciones\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cálculo simbólico de gradiente y hessiano\n",
    "    gradient = [sp.diff(f, xi) for xi in x]\n",
    "    hessian = sp.hessian(f, x)\n",
    "    \n",
    "    # Paso a numpy para posterior sustitución de valores\n",
    "    \n",
    "    gradient_func = sp.lambdify(x, gradient, 'numpy')\n",
    "    hessian_func = sp.lambdify(x, hessian, 'numpy')\n",
    "    \n",
    "    x0 = initial_guess\n",
    "    results = []\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        grad = gradient_func(*x0)\n",
    "        hess = hessian_func(*x0)\n",
    "        step = -np.dot(np.linalg.inv(hess), grad)\n",
    "        \n",
    "        results.append([iteration, np.round(x0, 4)])\n",
    "        \n",
    "        if np.linalg.norm(step) < tol:\n",
    "            break\n",
    "        \n",
    "        x0 = x0 + step\n",
    "        \n",
    "    table = pd.DataFrame(results)\n",
    "    table.columns = ['Iteration', 'x']\n",
    "    \n",
    "    return table\n",
    "\n",
    "\n",
    "# Llamada a función\n",
    "\n",
    "x, y = sp.symbols('x y')\n",
    "f = 3*x**2 - 2*x*y + y**2 + x\n",
    "\n",
    "initial_guess = [1, 1]\n",
    "\n",
    "result = newtons_method(f, (x, y), initial_guess)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método de la razón aúrea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def golden_ratio_search(func, lower, upper, tolerance, max_iter = 100):\n",
    "\n",
    "    golden_ratio = (math.sqrt(5) - 1) / 2\n",
    "\n",
    "    # Cálculo de subintervalos iniciales\n",
    "    a = lower\n",
    "    b = upper\n",
    "    x1 = a + (1 - golden_ratio) * (b - a)\n",
    "    x2 = a + golden_ratio * (b - a)\n",
    "\n",
    "    results = []\n",
    "    iter = 0\n",
    "    \n",
    "    results.append([iter, np.round(a, 4), np.round(b, 4)])\n",
    "    \n",
    "    while abs(b - a) > tolerance and (iter < max_iter):\n",
    "        if func(x1) < func(x2):\n",
    "            b = x2 # a no se ve modificado\n",
    "        else:\n",
    "            a = x1 # b no se ve modificado\n",
    "\n",
    "        x1 = a + (1 - golden_ratio) * (b - a)\n",
    "        x2 = a + golden_ratio * (b - a)\n",
    "        iter = iter + 1\n",
    "        \n",
    "        results.append([iter, np.round(a, 4), np.round(b, 4)])\n",
    "    \n",
    "    table = pd.DataFrame(results)\n",
    "    table.columns = ['Iteration', 'a', 'b']\n",
    "        \n",
    "    # Devolución del punto mínimo aproximado y del valor de la función ahí\n",
    "    min_point = (a + b) / 2\n",
    "    min_value = func(min_point)\n",
    "\n",
    "    return table, min_point, min_value, iter\n",
    "\n",
    "\n",
    "def test_function(x):\n",
    "    return -x * math.cos(x)\n",
    "\n",
    "lower_bound = 0\n",
    "upper_bound = math.pi/2\n",
    "tolerance = 1e-4\n",
    "\n",
    "results, min_point, min_value, iter = golden_ratio_search(test_function, lower_bound, upper_bound, tolerance)\n",
    "print(f\"Punto en el mínimo: {min_point}\")\n",
    "print(f\"Valor de la función en el mínimo: {min_value}\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método de Newton (1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "def newton_optimization_1d(f, x0, tol = 1e-4, max_iter = 100):\n",
    "    \n",
    "    x = sp.symbols('x')\n",
    "    f_expr = sp.sympify(f)\n",
    "    \n",
    "    # Cálculo de primera y segunda derivada\n",
    "    \n",
    "    f_prime = f_expr.diff(x)\n",
    "    f_double_prime = f_prime.diff(x)\n",
    "    \n",
    "    results = []\n",
    "    x_current = x0\n",
    "    iteration = 0\n",
    "    \n",
    "    results.append([iteration, x_current])\n",
    "    \n",
    "    while iteration < max_iter:\n",
    "        df_prime = f_prime.subs(x, x_current)\n",
    "        df_double_prime = f_double_prime.subs(x, x_current)\n",
    "        \n",
    "        x_new = x_current - df_prime / df_double_prime # iteración\n",
    "        \n",
    "        if abs(x_new - x_current) < tol: # salida por proximidad en iteraciones sucesivas para el punto\n",
    "            \n",
    "            # Criterio de segunda derivada (cuidado con algunas funciones)\n",
    "            \n",
    "            second_derivative = df_double_prime.subs(x, x_new)\n",
    "            \n",
    "            if second_derivative > 0:\n",
    "                print(f\"Se ha encontrado un mínimo.\")\n",
    "            elif second_derivative < 0:\n",
    "                print(f\"Se ha encontrado un máximo.\")\n",
    "            else:\n",
    "                print(f\"Según el criterio de la segunda derivada, no es máximo ni mínimo.\")\n",
    "            \n",
    "            break\n",
    "        \n",
    "        x_current = x_new\n",
    "        iteration += 1\n",
    "        results.append([iteration, x_current])\n",
    "        \n",
    "    table = pd.DataFrame(results)\n",
    "    table.columns = ['Iteration', 'x']\n",
    "        \n",
    "    return table\n",
    "\n",
    "# main \n",
    "\n",
    "x = sp.symbols('x')\n",
    "f = x ** 4 - 14*x**3 + 60*x**2 - 70*x\n",
    "x0 = 0.7 # valor inicial\n",
    "    \n",
    "results = newton_optimization_1d(f, x0, tol = 0.001, max_iter = 100)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método de la bisección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "def bisect_minimum(func, a, b, tol = 1e-6, max_iter = 100):\n",
    "    \"\"\"\n",
    "    Hallar el mínimo de una función de una sola variable en un intervalo [a, b] haciendo uso de su derivada.\n",
    "\n",
    "    Parámetros\n",
    "    func: función simbólica\n",
    "    a, b: extremo izquierdo y derecho del intervalo\n",
    "    tol: tolerancia\n",
    "    max_iter: número máximo de iteraciones\n",
    "    \"\"\"\n",
    "\n",
    "    x = sp.symbols('x')\n",
    "    derivative = sp.diff(func, x)\n",
    "    \n",
    "    derivative = sp.lambdify(x, derivative, 'numpy')\n",
    "\n",
    "    if derivative(a) * derivative(b) >= 0:\n",
    "        raise ValueError(\"Para aplicar el método de la bisección es necesario que las derivadas en los extremos tengan signo contrario.\")\n",
    "\n",
    "    iteration = 0\n",
    "    c = (a + b) / 2\n",
    "    results = []\n",
    "    results.append([iteration, c])\n",
    "    \n",
    "    while (b - a) / 2 > tol and iteration < max_iter:\n",
    "        \n",
    "        if derivative(c) == 0:\n",
    "            break\n",
    "        elif derivative(c) * derivative(a) < 0:\n",
    "            b = c\n",
    "        else:\n",
    "            a = c\n",
    "        \n",
    "        c = (a + b) / 2\n",
    "        iteration += 1\n",
    "        results.append([iteration, c])\n",
    "     \n",
    "    table = pd.DataFrame(results)\n",
    "    table.columns = ['Iteration', 'x']\n",
    "    \n",
    "    return table\n",
    "\n",
    "# Example usage:\n",
    "x = sp.symbols('x')\n",
    "func = x**4 - 14*x**3 + 60*x**2 - 70*x  # A simple quadratic function with a minimum at x = -1\n",
    "\n",
    "results = bisect_minimum(func, 0, 2)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método del gradiente conjugado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conjugate_gradient(A, b, x0, tol = 1e-6, max_iter = 100):\n",
    "    \n",
    "    x = x0\n",
    "    r = np.dot(A, x) - b \n",
    "    d = -r\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        \n",
    "        results.append([iteration, np.round(x, 4)])\n",
    "        \n",
    "        Ad = np.dot(A, d)\n",
    "        lamb = np.dot(r, r) / np.dot(d, Ad)\n",
    "        x = x + lamb * d\n",
    "        r_new = np.dot(A, x) - b\n",
    "        \n",
    "        if np.linalg.norm(r_new) < tol:\n",
    "            results.append([iteration + 1, np.round(x, 4)]) # x_{k + 1} retornado\n",
    "            break\n",
    "        \n",
    "        beta = np.dot(r_new, r_new) / np.dot(r, r)\n",
    "        d = -r_new + beta * d\n",
    "        r = r_new\n",
    "        \n",
    "    table = pd.DataFrame(results)\n",
    "    table.columns = ['Iteration', 'x']\n",
    "    return table\n",
    "\n",
    "# main\n",
    "A = np.array([[4, -1, 1], [-1, 4, -2], [1, -2, 4]]) # entre corchetes por filas\n",
    "b = np.array([12, -1, 5])\n",
    "x0 = np.array([0, 0, 0])\n",
    "\n",
    "results = conjugate_gradient(A, b, x0)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método del gradiente conjugado (no lineal) - Fletcher-Reeves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def backtrackingline(func, gradient, xk, direction, lamb = 0.5, c1 = 10e-4, max_iter = 100): \n",
    "    \n",
    "    def armijo_condition(lamb):\n",
    "        x_new = xk + lamb * direction\n",
    "        func_value = func(*x_new) \n",
    "        return (func_value <= func(*xk) + c1 * lamb * gradient.dot(direction))\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        if armijo_condition(lamb):\n",
    "            break\n",
    "        lamb = lamb / 2  # Reducción del paso si no se cumple la condición de Armijo\n",
    "\n",
    "    return lamb \n",
    "\n",
    "def fletcher_reeves(f, x, x0, tol = 1e-3, max_iterations = 100):\n",
    "    results = []\n",
    "    \n",
    "    gradient = [sp.diff(f, xi) for xi in x]\n",
    "    func = sp.lambdify(x, f, 'numpy')\n",
    "    gradient_func = sp.lambdify(x, gradient, 'numpy')\n",
    "    \n",
    "    gradient_point = gradient_func(*x0)\n",
    "    gradient_point = np.array(gradient_point)\n",
    "    d0 = -gradient_point\n",
    "    \n",
    "    # Inicialización de variables para el bucle\n",
    "    iteration = 0\n",
    "    x_new = x0\n",
    "    d_new = d0\n",
    "    gradient_point_new = gradient_point\n",
    "    results.append([iteration, np.round(x_new, 5)])\n",
    "    \n",
    "    while (np.linalg.norm(gradient_point) > tol and iteration < max_iterations):\n",
    "        \n",
    "        lamb = backtrackingline(func, gradient_point_new, x_new, d_new)\n",
    "        \n",
    "        gradient_point_old = gradient_func(*x_new) # Este es el gradiente evaluado en el x_k\n",
    "        \n",
    "        x_new = x_new + lamb*d_new # Aquí se pone el x_{k + 1}\n",
    "        \n",
    "        gradient_point_new = gradient_func(*x_new)\n",
    "        gradient_point_new = np.array(gradient_point_new)\n",
    "        \n",
    "        beta_new = np.dot(gradient_point_new, gradient_point_new) / np.dot(gradient_point_old, gradient_point_old)\n",
    "        d_new = -gradient_point_new + beta_new * d_new\n",
    "        \n",
    "        iteration = iteration + 1\n",
    "        results.append([iteration, np.round(x_new, 5)])\n",
    "        \n",
    "    table = pd.DataFrame(results)\n",
    "    table.columns = ['Iteration', 'x']\n",
    "    return table\n",
    "    \n",
    "# Llamada a función\n",
    "\n",
    "x, y = sp.symbols('x y')\n",
    "f = x - y + 2*x**2 + 2*x*y + y**2\n",
    "\n",
    "initial_guess = [0, 0]\n",
    "\n",
    "result = fletcher_reeves(f, (x, y), initial_guess)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "\n",
    "def levenberg_marquardt(func, indep_vars_symbols, params_symbols, indep_vars_values, y_vec, init_vals, tol = 1e-4, max_iter = 10, lambda_0 = 10):\n",
    "    \n",
    "    # Obtenemos la lista de funciones evaluados en todos los puntos\n",
    "    # de variables independientes\n",
    "    f = []\n",
    "    for idx in range(len(indep_vars_values[indep_vars_symbols[0]])):\n",
    "        subs_values = {key: values[idx] for key, values in indep_vars_values.items()}\n",
    "        f.append(func.subs(subs_values))\n",
    "    # Residuos\n",
    "    r = [f_j - y_j for f_j, y_j in zip(f, y_vec)]\n",
    "    # Jacobiano\n",
    "    jacobo = sp.Matrix(f).jacobian(params_symbols)\n",
    "\n",
    "    # Inicializamos variables para el bucle\n",
    "    results = []\n",
    "    params_k = init_vals\n",
    "    lambda_k = lambda_0\n",
    "    diff_cond = 2 * tol\n",
    "    iter_count = 0\n",
    "    last_iter = False\n",
    "\n",
    "    while iter_count < max_iter:\n",
    "\n",
    "        # Valores de la iteración k\n",
    "        jacobo_k = jacobo.subs(params_k)\n",
    "        r_k = np.asarray([r_j.subs(params_k) for r_j in r], dtype = np.float16)\n",
    "        d_k = - np.dot(\n",
    "            sp.Matrix(\n",
    "                np.dot(jacobo_k.T, jacobo_k) +\n",
    "                lambda_k * np.diag(np.dot(jacobo_k.T, jacobo_k).diagonal())).inv(),\n",
    "            np.dot(jacobo_k.T, r_k))\n",
    "        \n",
    "        # Valores de la iteración k + 1\n",
    "        params_k1 = {key_j: value_j + d_k_j for key_j, value_j, d_k_j in zip(params_k.keys(), params_k.values(), d_k)}\n",
    "        r_k1 = np.asarray([r_j.subs(params_k1) for r_j in r], dtype = np.float16)\n",
    "\n",
    "        # Añadimos la iteración a la lista de resultados\n",
    "        results.append([iter_count] + list(params_k.values()) + [sum(r_k**2), lambda_k] + list(d_k))\n",
    "\n",
    "        # Condición para el siguiente paso\n",
    "        lambda_cond = norm(r_k1) ** 2 - norm(r_k)**2\n",
    "        if lambda_cond > 0:\n",
    "            lambda_k *= 10\n",
    "        else:\n",
    "            lambda_k /= 10\n",
    "            params_k = params_k1\n",
    "            diff_cond = lambda_cond\n",
    "        iter_count += 1\n",
    "\n",
    "        # Condición de salida\n",
    "        if last_iter:\n",
    "            break\n",
    "        if abs(diff_cond) < tol:\n",
    "            last_iter = True\n",
    "\n",
    "    # Generamos tabla con los resultados\n",
    "    table = pd.DataFrame(results)\n",
    "    table.columns = ['Iter'] + [str(symbol) for symbol in params_symbols] + ['\\u2211 r²', '\\u03BB'] + ['d_' + str(symbol) for symbol in params_symbols]\n",
    "    \n",
    "    return table, params_k\n",
    "\n",
    "\n",
    "\n",
    "# Ejemplo diapositivas\n",
    "\n",
    "# Datos\n",
    "# Variables independientes\n",
    "t = sp.symbols('t')\n",
    "indep_vars_symbols = [t]\n",
    "indep_vars_values = {t: [0, 0.5, 1, 1.5, 2, 2.5, 3]}\n",
    "y_vec = [1.145, 0.512, 0.401, 0.054, 0.038, 0.014, 0.046]\n",
    "\n",
    "# Ecuación de ajuste\n",
    "# Parámetros\n",
    "x = sp.symbols('x')\n",
    "params_symbols = [x]\n",
    "func = np.e ** (- x * t)\n",
    "\n",
    "# Valores iniciales para los parámetros\n",
    "init_vals = {x: 4}\n",
    "\n",
    "# Llamamos a la función\n",
    "results_table, x_sol = levenberg_marquardt(func, indep_vars_symbols, params_symbols, indep_vars_values, y_vec, init_vals)\n",
    "results_table\n",
    "\n",
    "\n",
    "# Práctica 1: Ejercicio 2\n",
    "\n",
    "# Datos\n",
    "# Variables independientes\n",
    "t = sp.symbols('t')\n",
    "indep_vars_symbols = [t]\n",
    "indep_vars_values = {t: [1, 2, 3, 4, 5, 6, 7, 8]}\n",
    "y_vec = [8.3, 11.0, 14.7, 19.7, 26.7, 35.2, 44.4, 55.9]\n",
    "\n",
    "# Ecuación de ajuste\n",
    "# Parámetros\n",
    "x1, x2 = sp.symbols('x1 x2')\n",
    "params_symbols = [x1, x2]\n",
    "func = x1 * np.e ** (x2 * t)\n",
    "\n",
    "# Valores iniciales para los parámetros\n",
    "init_vals = {x1: 4, x2: 0.3}\n",
    "\n",
    "# Llamamos a la función\n",
    "results_table, x_sol = levenberg_marquardt(func, indep_vars_symbols, params_symbols, indep_vars_values, y_vec, init_vals)\n",
    "results_table\n",
    "\n",
    "# Ejercicio en el tema (2D)\n",
    "\n",
    "# Datos\n",
    "# Variables independientes\n",
    "t = sp.symbols('t')\n",
    "indep_vars_symbols = [t]\n",
    "\n",
    "indep_vars_values = {t: list(np.arange(-5, 5, 0.5))}\n",
    "y_vec = [-0.99999, -1.2090, -1.3877, -1.5090, -1.5510, -1.5, -1.3510, -1.109, -0.7877, -0.409, 0, 0.4090, 0.787786, 1.10901,\n",
    "         1.3510, 1.5, 1.5510, 1.5090, 1.3877, 1.209, 0.9999]\n",
    "\n",
    "# Ecuación de ajuste\n",
    "# Parámetros\n",
    "x1, x2 = sp.symbols('x1 x2')\n",
    "params_symbols = [x1, x2]\n",
    "func = sp.sin(x1 * t) + x2 * t\n",
    "\n",
    "# Valores iniciales para los parámetros\n",
    "init_vals = {x1: 0.5, x2: 0.3}\n",
    "\n",
    "# Llamamos a la función\n",
    "results_table, x_sol = levenberg_marquardt(func, indep_vars_symbols, params_symbols, indep_vars_values, y_vec, init_vals)\n",
    "results_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
