{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "# Armijo para obtener lambda\n",
    "def armijo_line_search(func, gradient, xk, direction, lamb = 0.5, c1 = 0.9, max_iter = 100):\n",
    "    \"\"\"\n",
    "    Armijo line search to find a suitable step size.\n",
    "\n",
    "    Parameters:\n",
    "    - func: The objective function to minimize.\n",
    "    - gradient: The gradient of the objective function.\n",
    "    - x: Current iterate.\n",
    "    - direction: Search direction.\n",
    "    - lamb: Initial step size.\n",
    "    - c1: Armijo condition parameter.\n",
    "    - max_iter: Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "    - lamb: The step size that satisfies Armijo-Wolfe conditions.\n",
    "    \"\"\"\n",
    "    def armijo_condition(lamb):\n",
    "        x_new = xk + lamb * direction\n",
    "        func_value = func(*x_new)  # Evaluate the symbolic function at x_new\n",
    "        return (func_value <= func(*xk) + c1 * lamb * gradient.dot(direction))\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        if armijo_condition(lamb):\n",
    "            break\n",
    "        lamb = lamb / 2  # Reduce step size if conditions are not met\n",
    "\n",
    "    return lamb  # Return the best step size found\n",
    "\n",
    "# Test de convergencia de descenso lineal\n",
    "def test_convergence_linear_descend(x0, x_new, func, gradient_point_new, tol1, tol2, tol3):\n",
    "\n",
    "    conditions = (np.linalg.norm(x0 - x_new) < tol1,\n",
    "                  np.linalg.norm(gradient_point_new) < tol2,\n",
    "                  abs(func(*x_new) - func(*x0)) < tol3)\n",
    "    \n",
    "    test = any(conditions)\n",
    "    condition_idx = np.where(conditions)[0]\n",
    "\n",
    "    return test, condition_idx\n",
    "\n",
    "# Método del gradiente\n",
    "def gradient_linear_descend(f, x, x0, tol1 = 1e-4, tol2 = 1e-4, tol3 = 1e-4, max_iterations = 100):\n",
    "    \n",
    "    func = sp.lambdify(x, f, 'numpy')\n",
    "\n",
    "    gradient = [sp.diff(f, xi) for xi in x]\n",
    "    gradient_func = sp.lambdify(x, gradient, 'numpy')\n",
    "    \n",
    "    gradient_point = gradient_func(*x0)\n",
    "    gradient_point = np.array(gradient_point)\n",
    "    d0 = -gradient_point\n",
    "    \n",
    "    # Inicialización de variables para el bucle\n",
    "    iteration = 0\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        print(x0)\n",
    "        print(iteration)\n",
    "        \n",
    "        lamb = armijo_line_search(func, gradient_point, x0, d0)\n",
    "        \n",
    "        x_new = x0 + lamb * d0 # Aquí se obtiene el x_{k + 1}\n",
    "        gradient_point_new = np.array(gradient_func(*x_new))\n",
    "        d_new = -gradient_point_new\n",
    "        \n",
    "        test, cond_idx = test_convergence_linear_descend(x0, x_new, func, gradient_point_new, tol1, tol2, tol3)\n",
    "        if test:\n",
    "            print(f\"Convergence reached in iteration {iteration}. Fullfilled conditions: {cond_idx}\")\n",
    "            break\n",
    "        else:\n",
    "            x0 = x_new\n",
    "            d0 = d_new\n",
    "            gradient_point = gradient_point_new\n",
    "            iteration = iteration + 1\n",
    "\n",
    "    return x_new, iteration\n",
    "    \n",
    "# Llamada a función\n",
    "\n",
    "x, y = sp.symbols('x y')\n",
    "f = 9*x**2 + 2*x*y + y**2\n",
    "\n",
    "initial_guess = [0, -1]\n",
    "\n",
    "result, iterations = gradient_linear_descend(f, (x, y), initial_guess)\n",
    "print(result)\n",
    "print(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "def newtons_method(f, x, initial_guess, tol = 1e-6, max_iter = 100):\n",
    "    \n",
    "    \"\"\"\n",
    "    Newton's method for finding the minimum of a multivariable function.\n",
    "    \n",
    "    Parameters:\n",
    "    f: function to minimize\n",
    "    x: list of sympy symbols\n",
    "    initial_guess: initial point to iterate\n",
    "    tol: float, optional\n",
    "        Tolerance for stopping criterion. The iteration stops when the norm\n",
    "        of the gradient is less than tol.\n",
    "    max_iter: int, optional\n",
    "        Maximum number of iterations.\n",
    "        \n",
    "    Returns:\n",
    "    x_0: array\n",
    "        The estimated minimum point as a list of numerical values.\n",
    "    iteration: int\n",
    "        The number of iterations performed.\n",
    "    \"\"\"\n",
    "    gradient = [sp.diff(f, xi) for xi in x]\n",
    "    hessian = sp.hessian(f, x)\n",
    "    gradient_func = sp.lambdify(x, gradient, 'numpy')\n",
    "    hessian_func = sp.lambdify(x, hessian, 'numpy')\n",
    "\n",
    "    x0 = initial_guess\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        print(x0)\n",
    "        print(iteration)\n",
    "        grad = gradient_func(*x0)\n",
    "        hess = hessian_func(*x0)\n",
    "        step = -np.dot(np.linalg.inv(hess), grad)\n",
    "        x0 = x0 + step\n",
    "        \n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "\n",
    "    return x0, iteration\n",
    "\n",
    "\n",
    "# Llamada a función\n",
    "\n",
    "x, y = sp.symbols('x y')\n",
    "f = 3*x**2 - 2*x*y + y**2 + x\n",
    "\n",
    "initial_guess = [1, 1]\n",
    "\n",
    "result, iterations = newtons_method(f, (x, y), initial_guess)\n",
    "print(result)\n",
    "print(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punto en el mínimo: 0.8603334800557735\n",
      "Valor de la función en el mínimo: -0.5610963381910328\n",
      "Número de iteraciones: 30\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def golden_ratio_search(func, lower, upper, tolerance):\n",
    "\n",
    "    golden_ratio = (math.sqrt(5) - 1) / 2\n",
    "\n",
    "    # Calculate initial values for the search\n",
    "    a = lower\n",
    "    b = upper\n",
    "    x1 = a + (1 - golden_ratio) * (b - a)\n",
    "    x2 = a + golden_ratio * (b - a)\n",
    "\n",
    "    iter = 0\n",
    "    \n",
    "    while abs(b - a) > tolerance:\n",
    "        if func(x1) < func(x2):\n",
    "            b = x2 # a no se ve modificado\n",
    "        else:\n",
    "            a = x1 # b no se ve modificado\n",
    "\n",
    "        x1 = a + (1 - golden_ratio) * (b - a)\n",
    "        x2 = a + golden_ratio * (b - a)\n",
    "        iter = iter + 1\n",
    "\n",
    "    # Return the approximate minimum point and value\n",
    "    min_point = (a + b) / 2\n",
    "    min_value = func(min_point)\n",
    "\n",
    "    return min_point, min_value, iter\n",
    "\n",
    "\n",
    "def test_function(x):\n",
    "    return -x * math.cos(x)\n",
    "\n",
    "lower_bound = 0\n",
    "upper_bound = math.pi/2\n",
    "tolerance = 1e-6\n",
    "\n",
    "min_point, min_value, iter = golden_ratio_search(test_function, lower_bound, upper_bound, tolerance)\n",
    "print(f\"Punto en el mínimo: {min_point}\")\n",
    "print(f\"Valor de la función en el mínimo: {min_value}\")\n",
    "print(f\"Número de iteraciones: {iter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum found at x = 0.780884053071772\n",
      "El mínimo está en: 0.780884053071772\n",
      "Number of iterations: {2}\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "def newton_optimization_1d(f, x0, tol = 1e-6, max_iter = 100):\n",
    "    \n",
    "    x = sp.symbols('x')\n",
    "    \n",
    "    f_expr = sp.sympify(f)\n",
    "    \n",
    "    f_prime = f_expr.diff(x)\n",
    "    f_double_prime = f_prime.diff(x)\n",
    "    \n",
    "    x_current = x0\n",
    "    iteration = 0\n",
    "    \n",
    "    while iteration < max_iter:\n",
    "        df_prime = f_prime.subs(x, x_current)\n",
    "        df_double_prime = f_double_prime.subs(x, x_current)\n",
    "        \n",
    "        x_new = x_current - df_prime / df_double_prime # iteración\n",
    "        \n",
    "        if abs(x_new - x_current) < tol: # salida por tolerancia\n",
    "            \n",
    "            second_derivative = df_double_prime.subs(x, x_new)\n",
    "            \n",
    "            if second_derivative > 0:\n",
    "                print(f\"Minimum found at x = {x_new}\")\n",
    "            elif second_derivative < 0:\n",
    "                print(f\"Maximum found at x = {x_new}\")\n",
    "            else:\n",
    "                print(f\"Saddle point found at x = {x_new}\")\n",
    "                \n",
    "            return x_new, iteration\n",
    "        \n",
    "        x_current = x_new\n",
    "        iteration += 1\n",
    "        \n",
    "    return x_current, iteration\n",
    "\n",
    "# main \n",
    "\n",
    "f = 'x ** 4 - 14*x**3 + 60*x**2 - 70*x'\n",
    "x0 = 0.7 # valor inicial\n",
    "    \n",
    "result, iter = newton_optimization_1d(f, x0, tol = 0.001, max_iter = 100)\n",
    "\n",
    "print(f\"El mínimo está en: {result}\")\n",
    "print(f\"Number of iterations:\", {iter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate minimum: 0.7808847427368164\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "def bisect_minimum(func, a, b, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Find the minimum of a function within the interval [a, b] using derivative information.\n",
    "\n",
    "    Parameters:\n",
    "    func (SymPy expression): The symbolic expression for the function.\n",
    "    a (float): Left endpoint of the interval.\n",
    "    b (float): Right endpoint of the interval.\n",
    "    tol (float): Tolerance for the minimum approximation.\n",
    "    max_iter (int): Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "    float: Approximation of the minimum.\n",
    "    \"\"\"\n",
    "\n",
    "    x = sp.symbols('x')\n",
    "    derivative = sp.diff(func, x)\n",
    "    \n",
    "    # Convert the symbolic functions to lambdas for numerical evaluation\n",
    "    func_lambda = sp.lambdify(x, func, 'numpy')\n",
    "    derivative_lambda = sp.lambdify(x, derivative, 'numpy')\n",
    "\n",
    "    if derivative_lambda(a) * derivative_lambda(b) >= 0:\n",
    "        raise ValueError(\"Derivative values at endpoints must have different signs.\")\n",
    "\n",
    "    iteration = 0\n",
    "\n",
    "    while (b - a) / 2 > tol and iteration < max_iter:\n",
    "        c = (a + b) / 2\n",
    "        if derivative_lambda(c) == 0:\n",
    "            return c\n",
    "        elif derivative_lambda(c) * derivative_lambda(a) < 0:\n",
    "            b = c\n",
    "        else:\n",
    "            a = c\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return (a + b) / 2\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    x = sp.symbols('x')\n",
    "    func = x**4 - 14*x**3 + 60*x**2 - 70*x  # A simple quadratic function with a minimum at x = -1\n",
    "\n",
    "    minimum = bisect_minimum(func, 0, 2)\n",
    "    print(\"Approximate minimum:\", minimum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conjugate_gradient(A, b, x0, tol = 1e-6, max_iter = 100):\n",
    "    \n",
    "    x = x0\n",
    "    r = np.dot(A, x) - b \n",
    "    d = -r\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        print(x)\n",
    "        \n",
    "        Ad = np.dot(A, d)\n",
    "        lamb = np.dot(r, r) / np.dot(d, Ad)\n",
    "        x = x + lamb * d\n",
    "        r_new = np.dot(A, x) - b\n",
    "        \n",
    "        if np.linalg.norm(r_new) < tol:\n",
    "            print(x)\n",
    "            break\n",
    "        \n",
    "        beta = np.dot(r_new, r_new) / np.dot(r, r)\n",
    "        d = -r_new + beta * d\n",
    "        r = r_new\n",
    "        \n",
    "    return x, iteration\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    A = np.array([[4, -1, 1], [-1, 4, -2], [1, -2, 4]]) # entre corchetes por filas\n",
    "    b = np.array([12, -1, 5])\n",
    "    x0 = np.array([0, 0, 0])\n",
    "\n",
    "    solution, iterations = conjugate_gradient(A, b, x0)\n",
    "    print(\"Solution:\", solution)\n",
    "    print(\"iteraciones\", iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5 -1. ]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "def armijo_line_search(func, gradient, xk, direction, lamb = 1, c1 = 0.6, max_iter = 100):\n",
    "    \"\"\"\n",
    "    Armijo line search to find a suitable step size.\n",
    "\n",
    "    Parameters:\n",
    "    - func: The objective function to minimize.\n",
    "    - gradient: The gradient of the objective function.\n",
    "    - x: Current iterate.\n",
    "    - direction: Search direction.\n",
    "    - lamb: Initial step size.\n",
    "    - c1: Armijo condition parameter.\n",
    "    - max_iter: Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "    - lamb: The step size that satisfies Armijo-Wolfe conditions.\n",
    "    \"\"\"\n",
    "    def armijo_condition(lamb):\n",
    "        x_new = xk + lamb * direction\n",
    "        func_value = func(*x_new)  # Evaluate the symbolic function at x_new\n",
    "        return (func_value <= func(*xk) + c1 * lamb * gradient.dot(direction))\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        if armijo_condition(lamb):\n",
    "            break\n",
    "        lamb = lamb / 2  # Reduce step size if conditions are not met\n",
    "\n",
    "    return lamb  # Return the best step size found\n",
    "\n",
    "def fletcher_reeves(f, x, x0, tol = 1e-4, max_iterations = 100):\n",
    "    \n",
    "    gradient = [sp.diff(f, xi) for xi in x]\n",
    "    func = sp.lambdify(x, f, 'numpy')\n",
    "    gradient_func = sp.lambdify(x, gradient, 'numpy')\n",
    "    \n",
    "    gradient_point = gradient_func(*x0)\n",
    "    gradient_point = np.array(gradient_point)\n",
    "    d0 = -gradient_point\n",
    "    \n",
    "    # Inicialización de variables para el bucle\n",
    "    iteration = 0\n",
    "    x_new = x0\n",
    "    d_new = d0\n",
    "    gradient_point_new = gradient_point\n",
    "    \n",
    "    while (np.linalg.norm(gradient_point) > tol and iteration < max_iterations):\n",
    "        \n",
    "        lamb = armijo_line_search(func, gradient_point_new, x_new, d_new)\n",
    "        \n",
    "        gradient_point_old = gradient_func(*x_new) # Este es el gradiente evaluado en el x_k\n",
    "        \n",
    "        x_new = x_new + lamb*d_new # Aquí se pone el x_{k + 1}\n",
    "        \n",
    "        gradient_point_new = gradient_func(*x_new)\n",
    "        gradient_point_new = np.array(gradient_point_new)\n",
    "        \n",
    "        beta_new = np.dot(gradient_point_new, gradient_point_new) / np.dot(gradient_point_old, gradient_point_old)\n",
    "        d_new = -gradient_point_new + beta_new * d_new\n",
    "        \n",
    "        iteration = iteration + 1\n",
    "    \n",
    "    return x_new, iteration\n",
    "    \n",
    "# Llamada a función\n",
    "\n",
    "x, y = sp.symbols('x y')\n",
    "f = (2*x - y)**2 + (y + 1)**2\n",
    "\n",
    "initial_guess = [5/2, 2]\n",
    "\n",
    "result, iterations = fletcher_reeves(f, (x, y), initial_guess)\n",
    "print(result)\n",
    "print(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.125, 0.125]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[-0.2207, 0.23633]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[-0.33636, 0.38853]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[-0.45221, 0.56147]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>[-1.0, 1.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>[-1.0, 1.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>[-1.0, 1.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>[-1.0, 1.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100</td>\n",
       "      <td>[-1.0, 1.5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Iteration                    x\n",
       "0            0               [0, 0]\n",
       "1            1      [-0.125, 0.125]\n",
       "2            2   [-0.2207, 0.23633]\n",
       "3            3  [-0.33636, 0.38853]\n",
       "4            4  [-0.45221, 0.56147]\n",
       "..         ...                  ...\n",
       "96          96          [-1.0, 1.5]\n",
       "97          97          [-1.0, 1.5]\n",
       "98          98          [-1.0, 1.5]\n",
       "99          99          [-1.0, 1.5]\n",
       "100        100          [-1.0, 1.5]\n",
       "\n",
       "[101 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def armijo_line_search(func, gradient, xk, direction, lamb = 0.5, c1 = 0.9, max_iter = 100):\n",
    "    \"\"\"\n",
    "    Armijo line search to find a suitable step size.\n",
    "\n",
    "    Parameters:\n",
    "    - func: The objective function to minimize.\n",
    "    - gradient: The gradient of the objective function.\n",
    "    - x: Current iterate.\n",
    "    - direction: Search direction.\n",
    "    - lamb: Initial step size.\n",
    "    - c1: Armijo condition parameter.\n",
    "    - max_iter: Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "    - lamb: The step size that satisfies Armijo-Wolfe conditions.\n",
    "    \"\"\"\n",
    "    def armijo_condition(lamb):\n",
    "        x_new = xk + lamb * direction\n",
    "        func_value = func(*x_new)  # Evaluate the symbolic function at x_new\n",
    "        return (func_value <= func(*xk) + c1 * lamb * gradient.dot(direction))\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        if armijo_condition(lamb):\n",
    "            break\n",
    "        lamb = lamb / 2  # Reduce step size if conditions are not met\n",
    "\n",
    "    return lamb  # Return the best step size found\n",
    "\n",
    "def fletcher_reeves(f, x, x0, tol = 1e-3, max_iterations = 100):\n",
    "    results = []\n",
    "    \n",
    "    gradient = [sp.diff(f, xi) for xi in x]\n",
    "    func = sp.lambdify(x, f, 'numpy')\n",
    "    gradient_func = sp.lambdify(x, gradient, 'numpy')\n",
    "    \n",
    "    gradient_point = gradient_func(*x0)\n",
    "    gradient_point = np.array(gradient_point)\n",
    "    d0 = -gradient_point\n",
    "    \n",
    "    # Inicialización de variables para el bucle\n",
    "    iteration = 0\n",
    "    x_new = x0\n",
    "    d_new = d0\n",
    "    gradient_point_new = gradient_point\n",
    "    results.append([iteration, np.round(x_new, 5)])\n",
    "    \n",
    "    while (np.linalg.norm(gradient_point) > tol and iteration < max_iterations):\n",
    "        \n",
    "        lamb = armijo_line_search(func, gradient_point_new, x_new, d_new)\n",
    "        \n",
    "        gradient_point_old = gradient_func(*x_new) # Este es el gradiente evaluado en el x_k\n",
    "        \n",
    "        x_new = x_new + lamb*d_new # Aquí se pone el x_{k + 1}\n",
    "        \n",
    "        gradient_point_new = gradient_func(*x_new)\n",
    "        gradient_point_new = np.array(gradient_point_new)\n",
    "        \n",
    "        beta_new = np.dot(gradient_point_new, gradient_point_new) / np.dot(gradient_point_old, gradient_point_old)\n",
    "        d_new = -gradient_point_new + beta_new * d_new\n",
    "        \n",
    "        iteration = iteration + 1\n",
    "        results.append([iteration, np.round(x_new, 5)])\n",
    "        \n",
    "    table = pd.DataFrame(results)\n",
    "    table.columns = ['Iteration', 'x']\n",
    "    return table\n",
    "    \n",
    "# Llamada a función\n",
    "\n",
    "x, y = sp.symbols('x y')\n",
    "f = x - y + 2*x**2 + 2*x*y + y**2\n",
    "\n",
    "initial_guess = [0, 0]\n",
    "\n",
    "result = fletcher_reeves(f, (x, y), initial_guess)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iter</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>∑ r²</th>\n",
       "      <th>λ</th>\n",
       "      <th>d_x1</th>\n",
       "      <th>d_x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.825712e+00</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.0221326872632251</td>\n",
       "      <td>-0.0139062009314106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.522132687263225</td>\n",
       "      <td>0.286093799068589</td>\n",
       "      <td>3.600886e+00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0779176070662545</td>\n",
       "      <td>-0.0553679622604664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.600050294329480</td>\n",
       "      <td>0.230725836808123</td>\n",
       "      <td>4.011502e-01</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0285334504435690</td>\n",
       "      <td>-0.0285468002426872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.628583744773048</td>\n",
       "      <td>0.202179036565436</td>\n",
       "      <td>6.914735e-04</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>-0.000204540904874094</td>\n",
       "      <td>-0.00213121339931450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.628379203868174</td>\n",
       "      <td>0.200047823166121</td>\n",
       "      <td>1.788139e-07</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>-6.88233544527046e-5</td>\n",
       "      <td>-6.29256029316954e-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.628310380513722</td>\n",
       "      <td>0.199984897563190</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-3.75860888568494e-7</td>\n",
       "      <td>-2.87385368397551e-7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Iter                 x1                 x2          ∑ r²        λ  \\\n",
       "0     0                0.5                0.3  4.825712e+00  10.0000   \n",
       "1     1  0.522132687263225  0.286093799068589  3.600886e+00   1.0000   \n",
       "2     2  0.600050294329480  0.230725836808123  4.011502e-01   0.1000   \n",
       "3     3  0.628583744773048  0.202179036565436  6.914735e-04   0.0100   \n",
       "4     4  0.628379203868174  0.200047823166121  1.788139e-07   0.0010   \n",
       "5     5  0.628310380513722  0.199984897563190  0.000000e+00   0.0001   \n",
       "\n",
       "                    d_x1                  d_x2  \n",
       "0     0.0221326872632251   -0.0139062009314106  \n",
       "1     0.0779176070662545   -0.0553679622604664  \n",
       "2     0.0285334504435690   -0.0285468002426872  \n",
       "3  -0.000204540904874094  -0.00213121339931450  \n",
       "4   -6.88233544527046e-5  -6.29256029316954e-5  \n",
       "5   -3.75860888568494e-7  -2.87385368397551e-7  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "def levenberg_marquardt(func, indep_vars_symbols, params_symbols, indep_vars_values, y_vec, init_vals, tol = 1e-4, max_iter = 10, lambda_0 = 10):\n",
    "    \n",
    "    # Obtenemos la lista de funciones evaluados en todos los puntos\n",
    "    # de variables independientes\n",
    "    f = []\n",
    "    for idx in range(len(indep_vars_values[indep_vars_symbols[0]])):\n",
    "        subs_values = {key: values[idx] for key, values in indep_vars_values.items()}\n",
    "        f.append(func.subs(subs_values))\n",
    "    # Residuos\n",
    "    r = [f_j - y_j for f_j, y_j in zip(f, y_vec)]\n",
    "    # Jacobiano\n",
    "    jacobo = sp.Matrix(f).jacobian(params_symbols)\n",
    "\n",
    "    # Inicializamos variables para el bucle\n",
    "    results = []\n",
    "    params_k = init_vals\n",
    "    lambda_k = lambda_0\n",
    "    diff_cond = 2 * tol\n",
    "    iter_count = 0\n",
    "    last_iter = False\n",
    "\n",
    "    while iter_count < max_iter:\n",
    "\n",
    "        # Valores de la iteración k\n",
    "        jacobo_k = jacobo.subs(params_k)\n",
    "        r_k = np.asarray([r_j.subs(params_k) for r_j in r], dtype = np.float16)\n",
    "        d_k = - np.dot(\n",
    "            sp.Matrix(\n",
    "                np.dot(jacobo_k.T, jacobo_k) +\n",
    "                lambda_k * np.diag(np.dot(jacobo_k.T, jacobo_k).diagonal())).inv(),\n",
    "            np.dot(jacobo_k.T, r_k))\n",
    "        \n",
    "        # Valores de la iteración k + 1\n",
    "        params_k1 = {key_j: value_j + d_k_j for key_j, value_j, d_k_j in zip(params_k.keys(), params_k.values(), d_k)}\n",
    "        r_k1 = np.asarray([r_j.subs(params_k1) for r_j in r], dtype = np.float16)\n",
    "\n",
    "        # Añadimos la iteración a la lista de resultados\n",
    "        results.append([iter_count] + list(params_k.values()) + [sum(r_k**2), lambda_k] + list(d_k))\n",
    "\n",
    "        # Condición para el siguiente paso\n",
    "        lambda_cond = norm(r_k1) ** 2 - norm(r_k)**2\n",
    "        if lambda_cond > 0:\n",
    "            lambda_k *= 10\n",
    "        else:\n",
    "            lambda_k /= 10\n",
    "            params_k = params_k1\n",
    "            diff_cond = lambda_cond\n",
    "        iter_count += 1\n",
    "\n",
    "        # Condición de salida\n",
    "        if last_iter:\n",
    "            break\n",
    "        if abs(diff_cond) < tol:\n",
    "            last_iter = True\n",
    "\n",
    "    # Generamos tabla con los resultados\n",
    "    table = pd.DataFrame(results)\n",
    "    table.columns = ['Iter'] + [str(symbol) for symbol in params_symbols] + ['\\u2211 r²', '\\u03BB'] + ['d_' + str(symbol) for symbol in params_symbols]\n",
    "    \n",
    "    return table, params_k\n",
    "\n",
    "\n",
    "\n",
    "# Ejemplo diapositivas\n",
    "\n",
    "# Datos\n",
    "# Variables independientes\n",
    "t = sp.symbols('t')\n",
    "indep_vars_symbols = [t]\n",
    "indep_vars_values = {t: [0, 0.5, 1, 1.5, 2, 2.5, 3]}\n",
    "y_vec = [1.145, 0.512, 0.401, 0.054, 0.038, 0.014, 0.046]\n",
    "\n",
    "# Ecuación de ajuste\n",
    "# Parámetros\n",
    "x = sp.symbols('x')\n",
    "params_symbols = [x]\n",
    "func = np.e ** (- x * t)\n",
    "\n",
    "# Valores iniciales para los parámetros\n",
    "init_vals = {x: 4}\n",
    "\n",
    "# Llamamos a la función\n",
    "results_table, x_sol = levenberg_marquardt(func, indep_vars_symbols, params_symbols, indep_vars_values, y_vec, init_vals)\n",
    "results_table\n",
    "\n",
    "\n",
    "# Práctica 1: Ejercicio 2\n",
    "\n",
    "# Datos\n",
    "# Variables independientes\n",
    "t = sp.symbols('t')\n",
    "indep_vars_symbols = [t]\n",
    "indep_vars_values = {t: [1, 2, 3, 4, 5, 6, 7, 8]}\n",
    "y_vec = [8.3, 11.0, 14.7, 19.7, 26.7, 35.2, 44.4, 55.9]\n",
    "\n",
    "# Ecuación de ajuste\n",
    "# Parámetros\n",
    "x1, x2 = sp.symbols('x1 x2')\n",
    "params_symbols = [x1, x2]\n",
    "func = x1 * np.e ** (x2 * t)\n",
    "\n",
    "# Valores iniciales para los parámetros\n",
    "init_vals = {x1: 4, x2: 0.3}\n",
    "\n",
    "# Llamamos a la función\n",
    "results_table, x_sol = levenberg_marquardt(func, indep_vars_symbols, params_symbols, indep_vars_values, y_vec, init_vals)\n",
    "results_table\n",
    "\n",
    "# Ejercicio en el tema (2D)\n",
    "\n",
    "# Datos\n",
    "# Variables independientes\n",
    "t = sp.symbols('t')\n",
    "indep_vars_symbols = [t]\n",
    "\n",
    "indep_vars_values = {t: list(np.arange(-5, 5, 0.5))}\n",
    "y_vec = [-0.99999, -1.2090, -1.3877, -1.5090, -1.5510, -1.5, -1.3510, -1.109, -0.7877, -0.409, 0, 0.4090, 0.787786, 1.10901,\n",
    "         1.3510, 1.5, 1.5510, 1.5090, 1.3877, 1.209, 0.9999]\n",
    "\n",
    "# Ecuación de ajuste\n",
    "# Parámetros\n",
    "x1, x2 = sp.symbols('x1 x2')\n",
    "params_symbols = [x1, x2]\n",
    "func = sp.sin(x1 * t) + x2 * t\n",
    "\n",
    "# Valores iniciales para los parámetros\n",
    "init_vals = {x1: 0.5, x2: 0.3}\n",
    "\n",
    "# Llamamos a la función\n",
    "results_table, x_sol = levenberg_marquardt(func, indep_vars_symbols, params_symbols, indep_vars_values, y_vec, init_vals)\n",
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.0, -4.5, -4.0, -3.5, -3.0, -2.5, -2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n",
      "[-5.0, -4.5, -4.0, -3.5, -3.0, -2.5, -2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a NumPy array\n",
    "numpy_array = np.linspace(-5, 5, num = 21)\n",
    "\n",
    "# Convert the NumPy array to a Python list\n",
    "python_list = numpy_array.tolist()\n",
    "\n",
    "# Print the Python list\n",
    "print(python_list)\n",
    "\n",
    "print(list(np.arange(-5, 5, 0.5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
