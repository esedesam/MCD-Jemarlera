{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "# Armijo para obtener lambda\n",
    "def armijo_line_search(func, gradient, xk, direction, lamb = 0.5, c1 = 0.9, max_iter = 100):\n",
    "    \"\"\"\n",
    "    Armijo line search to find a suitable step size.\n",
    "\n",
    "    Parameters:\n",
    "    - func: The objective function to minimize.\n",
    "    - gradient: The gradient of the objective function.\n",
    "    - x: Current iterate.\n",
    "    - direction: Search direction.\n",
    "    - lamb: Initial step size.\n",
    "    - c1: Armijo condition parameter.\n",
    "    - max_iter: Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "    - lamb: The step size that satisfies Armijo-Wolfe conditions.\n",
    "    \"\"\"\n",
    "    def armijo_condition(lamb):\n",
    "        x_new = xk + lamb * direction\n",
    "        func_value = func(*x_new)  # Evaluate the symbolic function at x_new\n",
    "        return (func_value <= func(*xk) + c1 * lamb * gradient.dot(direction))\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        if armijo_condition(lamb):\n",
    "            break\n",
    "        lamb = lamb / 2  # Reduce step size if conditions are not met\n",
    "\n",
    "    return lamb  # Return the best step size found\n",
    "\n",
    "# Test de convergencia de descenso lineal\n",
    "def test_convergence_linear_descend(x0, x_new, func, gradient_point_new, tol1, tol2, tol3):\n",
    "\n",
    "    conditions = (np.linalg.norm(x0 - x_new) < tol1,\n",
    "                  np.linalg.norm(gradient_point_new) < tol2,\n",
    "                  abs(func(*x_new) - func(*x0)) < tol3)\n",
    "    \n",
    "    test = any(conditions)\n",
    "    condition_idx = np.where(conditions)[0]\n",
    "\n",
    "    return test, condition_idx\n",
    "\n",
    "# Método del gradiente\n",
    "def gradient_linear_descend(f, x, x0, tol1 = 1e-4, tol2 = 1e-4, tol3 = 1e-4, max_iterations = 100):\n",
    "    \n",
    "    func = sp.lambdify(x, f, 'numpy')\n",
    "\n",
    "    gradient = [sp.diff(f, xi) for xi in x]\n",
    "    gradient_func = sp.lambdify(x, gradient, 'numpy')\n",
    "    \n",
    "    gradient_point = gradient_func(*x0)\n",
    "    gradient_point = np.array(gradient_point)\n",
    "    d0 = -gradient_point\n",
    "    \n",
    "    # Inicialización de variables para el bucle\n",
    "    iteration = 0\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        print(x0)\n",
    "        print(iteration)\n",
    "        \n",
    "        lamb = armijo_line_search(func, gradient_point, x0, d0)\n",
    "        \n",
    "        x_new = x0 + lamb * d0 # Aquí se obtiene el x_{k + 1}\n",
    "        gradient_point_new = np.array(gradient_func(*x_new))\n",
    "        d_new = -gradient_point_new\n",
    "        \n",
    "        test, cond_idx = test_convergence_linear_descend(x0, x_new, func, gradient_point_new, tol1, tol2, tol3)\n",
    "        if test:\n",
    "            print(f\"Convergence reached in iteration {iteration}. Fullfilled conditions: {cond_idx}\")\n",
    "            break\n",
    "        else:\n",
    "            x0 = x_new\n",
    "            d0 = d_new\n",
    "            gradient_point = gradient_point_new\n",
    "            iteration = iteration + 1\n",
    "\n",
    "    return x_new, iteration\n",
    "    \n",
    "# Llamada a función\n",
    "\n",
    "x, y = sp.symbols('x y')\n",
    "f = 9*x**2 + 2*x*y + y**2\n",
    "\n",
    "initial_guess = [0, -1]\n",
    "\n",
    "result, iterations = gradient_linear_descend(f, (x, y), initial_guess)\n",
    "print(result)\n",
    "print(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "def newtons_method(f, x, initial_guess, tol = 1e-6, max_iter = 100):\n",
    "    \n",
    "    \"\"\"\n",
    "    Newton's method for finding the minimum of a multivariable function.\n",
    "    \n",
    "    Parameters:\n",
    "    f: function to minimize\n",
    "    x: list of sympy symbols\n",
    "    initial_guess: initial point to iterate\n",
    "    tol: float, optional\n",
    "        Tolerance for stopping criterion. The iteration stops when the norm\n",
    "        of the gradient is less than tol.\n",
    "    max_iter: int, optional\n",
    "        Maximum number of iterations.\n",
    "        \n",
    "    Returns:\n",
    "    x_0: array\n",
    "        The estimated minimum point as a list of numerical values.\n",
    "    iteration: int\n",
    "        The number of iterations performed.\n",
    "    \"\"\"\n",
    "    gradient = [sp.diff(f, xi) for xi in x]\n",
    "    hessian = sp.hessian(f, x)\n",
    "    gradient_func = sp.lambdify(x, gradient, 'numpy')\n",
    "    hessian_func = sp.lambdify(x, hessian, 'numpy')\n",
    "\n",
    "    x0 = initial_guess\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        print(x0)\n",
    "        print(iteration)\n",
    "        grad = gradient_func(*x0)\n",
    "        hess = hessian_func(*x0)\n",
    "        step = -np.dot(np.linalg.inv(hess), grad)\n",
    "        x0 = x0 + step\n",
    "        \n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "\n",
    "    return x0, iteration\n",
    "\n",
    "\n",
    "# Llamada a función\n",
    "\n",
    "x, y = sp.symbols('x y')\n",
    "f = 3*x**2 - 2*x*y + y**2 + x\n",
    "\n",
    "initial_guess = [1, 1]\n",
    "\n",
    "result, iterations = newtons_method(f, (x, y), initial_guess)\n",
    "print(result)\n",
    "print(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punto en el mínimo: 0.8603334800557735\n",
      "Valor de la función en el mínimo: -0.5610963381910328\n",
      "Número de iteraciones: 30\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def golden_ratio_search(func, lower, upper, tolerance):\n",
    "\n",
    "    golden_ratio = (math.sqrt(5) - 1) / 2\n",
    "\n",
    "    # Calculate initial values for the search\n",
    "    a = lower\n",
    "    b = upper\n",
    "    x1 = a + (1 - golden_ratio) * (b - a)\n",
    "    x2 = a + golden_ratio * (b - a)\n",
    "\n",
    "    iter = 0\n",
    "    \n",
    "    while abs(b - a) > tolerance:\n",
    "        if func(x1) < func(x2):\n",
    "            b = x2 # a no se ve modificado\n",
    "        else:\n",
    "            a = x1 # b no se ve modificado\n",
    "\n",
    "        x1 = a + (1 - golden_ratio) * (b - a)\n",
    "        x2 = a + golden_ratio * (b - a)\n",
    "        iter = iter + 1\n",
    "\n",
    "    # Return the approximate minimum point and value\n",
    "    min_point = (a + b) / 2\n",
    "    min_value = func(min_point)\n",
    "\n",
    "    return min_point, min_value, iter\n",
    "\n",
    "\n",
    "def test_function(x):\n",
    "    return -x * math.cos(x)\n",
    "\n",
    "lower_bound = 0\n",
    "upper_bound = math.pi/2\n",
    "tolerance = 1e-6\n",
    "\n",
    "min_point, min_value, iter = golden_ratio_search(test_function, lower_bound, upper_bound, tolerance)\n",
    "print(f\"Punto en el mínimo: {min_point}\")\n",
    "print(f\"Valor de la función en el mínimo: {min_value}\")\n",
    "print(f\"Número de iteraciones: {iter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum found at x = 0.780884053071772\n",
      "El mínimo está en: 0.780884053071772\n",
      "Number of iterations: {2}\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "def newton_optimization_1d(f, x0, tol = 1e-6, max_iter = 100):\n",
    "    \n",
    "    x = sp.symbols('x')\n",
    "    \n",
    "    f_expr = sp.sympify(f)\n",
    "    \n",
    "    f_prime = f_expr.diff(x)\n",
    "    f_double_prime = f_prime.diff(x)\n",
    "    \n",
    "    x_current = x0\n",
    "    iteration = 0\n",
    "    \n",
    "    while iteration < max_iter:\n",
    "        df_prime = f_prime.subs(x, x_current)\n",
    "        df_double_prime = f_double_prime.subs(x, x_current)\n",
    "        \n",
    "        x_new = x_current - df_prime / df_double_prime # iteración\n",
    "        \n",
    "        if abs(x_new - x_current) < tol: # salida por tolerancia\n",
    "            \n",
    "            second_derivative = df_double_prime.subs(x, x_new)\n",
    "            \n",
    "            if second_derivative > 0:\n",
    "                print(f\"Minimum found at x = {x_new}\")\n",
    "            elif second_derivative < 0:\n",
    "                print(f\"Maximum found at x = {x_new}\")\n",
    "            else:\n",
    "                print(f\"Saddle point found at x = {x_new}\")\n",
    "                \n",
    "            return x_new, iteration\n",
    "        \n",
    "        x_current = x_new\n",
    "        iteration += 1\n",
    "        \n",
    "    return x_current, iteration\n",
    "\n",
    "# main \n",
    "\n",
    "f = 'x ** 4 - 14*x**3 + 60*x**2 - 70*x'\n",
    "x0 = 0.7 # valor inicial\n",
    "    \n",
    "result, iter = newton_optimization_1d(f, x0, tol = 0.001, max_iter = 100)\n",
    "\n",
    "print(f\"El mínimo está en: {result}\")\n",
    "print(f\"Number of iterations:\", {iter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate minimum: 0.7808847427368164\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "def bisect_minimum(func, a, b, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Find the minimum of a function within the interval [a, b] using derivative information.\n",
    "\n",
    "    Parameters:\n",
    "    func (SymPy expression): The symbolic expression for the function.\n",
    "    a (float): Left endpoint of the interval.\n",
    "    b (float): Right endpoint of the interval.\n",
    "    tol (float): Tolerance for the minimum approximation.\n",
    "    max_iter (int): Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "    float: Approximation of the minimum.\n",
    "    \"\"\"\n",
    "\n",
    "    x = sp.symbols('x')\n",
    "    derivative = sp.diff(func, x)\n",
    "    \n",
    "    # Convert the symbolic functions to lambdas for numerical evaluation\n",
    "    func_lambda = sp.lambdify(x, func, 'numpy')\n",
    "    derivative_lambda = sp.lambdify(x, derivative, 'numpy')\n",
    "\n",
    "    if derivative_lambda(a) * derivative_lambda(b) >= 0:\n",
    "        raise ValueError(\"Derivative values at endpoints must have different signs.\")\n",
    "\n",
    "    iteration = 0\n",
    "\n",
    "    while (b - a) / 2 > tol and iteration < max_iter:\n",
    "        c = (a + b) / 2\n",
    "        if derivative_lambda(c) == 0:\n",
    "            return c\n",
    "        elif derivative_lambda(c) * derivative_lambda(a) < 0:\n",
    "            b = c\n",
    "        else:\n",
    "            a = c\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return (a + b) / 2\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    x = sp.symbols('x')\n",
    "    func = x**4 - 14*x**3 + 60*x**2 - 70*x  # A simple quadratic function with a minimum at x = -1\n",
    "\n",
    "    minimum = bisect_minimum(func, 0, 2)\n",
    "    print(\"Approximate minimum:\", minimum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conjugate_gradient(A, b, x0, tol = 1e-6, max_iter = 100):\n",
    "    \n",
    "    x = x0\n",
    "    r = np.dot(A, x) - b \n",
    "    d = -r\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        print(x)\n",
    "        \n",
    "        Ad = np.dot(A, d)\n",
    "        lamb = np.dot(r, r) / np.dot(d, Ad)\n",
    "        x = x + lamb * d\n",
    "        r_new = np.dot(A, x) - b\n",
    "        \n",
    "        if np.linalg.norm(r_new) < tol:\n",
    "            print(x)\n",
    "            break\n",
    "        \n",
    "        beta = np.dot(r_new, r_new) / np.dot(r, r)\n",
    "        d = -r_new + beta * d\n",
    "        r = r_new\n",
    "        \n",
    "    return x, iteration\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    A = np.array([[4, -1, 1], [-1, 4, -2], [1, -2, 4]]) # entre corchetes por filas\n",
    "    b = np.array([12, -1, 5])\n",
    "    x0 = np.array([0, 0, 0])\n",
    "\n",
    "    solution, iterations = conjugate_gradient(A, b, x0)\n",
    "    print(\"Solution:\", solution)\n",
    "    print(\"iteraciones\", iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5 -1. ]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "def armijo_line_search(func, gradient, xk, direction, lamb = 1, c1 = 0.6, max_iter = 100):\n",
    "    \"\"\"\n",
    "    Armijo line search to find a suitable step size.\n",
    "\n",
    "    Parameters:\n",
    "    - func: The objective function to minimize.\n",
    "    - gradient: The gradient of the objective function.\n",
    "    - x: Current iterate.\n",
    "    - direction: Search direction.\n",
    "    - lamb: Initial step size.\n",
    "    - c1: Armijo condition parameter.\n",
    "    - max_iter: Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "    - lamb: The step size that satisfies Armijo-Wolfe conditions.\n",
    "    \"\"\"\n",
    "    def armijo_condition(lamb):\n",
    "        x_new = xk + lamb * direction\n",
    "        func_value = func(*x_new)  # Evaluate the symbolic function at x_new\n",
    "        return (func_value <= func(*xk) + c1 * lamb * gradient.dot(direction))\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        if armijo_condition(lamb):\n",
    "            break\n",
    "        lamb = lamb / 2  # Reduce step size if conditions are not met\n",
    "\n",
    "    return lamb  # Return the best step size found\n",
    "\n",
    "def fletcher_reeves(f, x, x0, tol = 1e-4, max_iterations = 100):\n",
    "    \n",
    "    gradient = [sp.diff(f, xi) for xi in x]\n",
    "    func = sp.lambdify(x, f, 'numpy')\n",
    "    gradient_func = sp.lambdify(x, gradient, 'numpy')\n",
    "    \n",
    "    gradient_point = gradient_func(*x0)\n",
    "    gradient_point = np.array(gradient_point)\n",
    "    d0 = -gradient_point\n",
    "    \n",
    "    # Inicialización de variables para el bucle\n",
    "    iteration = 0\n",
    "    x_new = x0\n",
    "    d_new = d0\n",
    "    gradient_point_new = gradient_point\n",
    "    \n",
    "    while (np.linalg.norm(gradient_point) > tol and iteration < max_iterations):\n",
    "        \n",
    "        lamb = armijo_line_search(func, gradient_point_new, x_new, d_new)\n",
    "        \n",
    "        gradient_point_old = gradient_func(*x_new) # Este es el gradiente evaluado en el x_k\n",
    "        \n",
    "        x_new = x_new + lamb*d_new # Aquí se pone el x_{k + 1}\n",
    "        \n",
    "        gradient_point_new = gradient_func(*x_new)\n",
    "        gradient_point_new = np.array(gradient_point_new)\n",
    "        \n",
    "        beta_new = np.dot(gradient_point_new, gradient_point_new) / np.dot(gradient_point_old, gradient_point_old)\n",
    "        d_new = -gradient_point_new + beta_new * d_new\n",
    "        \n",
    "        iteration = iteration + 1\n",
    "    \n",
    "    return x_new, iteration\n",
    "    \n",
    "# Llamada a función\n",
    "\n",
    "x, y = sp.symbols('x y')\n",
    "f = (2*x - y)**2 + (y + 1)**2\n",
    "\n",
    "initial_guess = [5/2, 2]\n",
    "\n",
    "result, iterations = fletcher_reeves(f, (x, y), initial_guess)\n",
    "print(result)\n",
    "print(iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
